{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcT80cvPJTMw"
      },
      "outputs": [],
      "source": [
        "#clean data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0nFh59eyNYsX",
        "outputId": "255cd25b-a466-4ef1-cf46-e46c2367a548"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.11.15)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.4.26)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m106.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, torch_geometric, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torch_geometric-2.6.1\n"
          ]
        }
      ],
      "source": [
        "pip install torch torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "collapsed": true,
        "id": "8nbz95A8IW_a",
        "outputId": "c20c2a60-6a45-4519-b506-852f3b046a59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload your CSV file containing company data:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7befce84-7626-4cf7-855f-f7f871508cd0\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7befce84-7626-4cf7-855f-f7f871508cd0\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving ind_nifty500list_filtered_final.csv to ind_nifty500list_filtered_final.csv\n",
            "Uploaded file: ind_nifty500list_filtered_final.csv\n",
            "YF.download() has changed argument auto_adjust default to True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n",
            "ERROR:yfinance:\n",
            "1 Failed download:\n",
            "ERROR:yfinance:['ONGC.NS']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to fetch reference data.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# Manually Upload CSV Instead of Using Kaggle\n",
        "print(\"Upload your CSV file containing company data:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get the uploaded file name\n",
        "csv_filename = list(uploaded.keys())[0]\n",
        "print(f\"Uploaded file: {csv_filename}\")\n",
        "\n",
        "def load_company_data(filepath):\n",
        "    df = pd.read_csv(filepath)\n",
        "\n",
        "    if 'Symbol' not in df.columns or 'Industry' not in df.columns:\n",
        "        raise ValueError(\"CSV file must contain 'Symbol' and 'Industry' columns.\")\n",
        "\n",
        "    label_encoder = LabelEncoder()\n",
        "    df['Industry Encoded'] = label_encoder.fit_transform(df['Industry'])\n",
        "    return df['Symbol'].tolist(), df['Industry'].tolist(), df['Industry Encoded'].tolist()\n",
        "\n",
        "def fetch_stock_data(ticker, start_date, end_date):\n",
        "    try:\n",
        "        data = yf.download(f\"{ticker}.NS\", start=start_date, end=end_date)\n",
        "        return data if not data.empty else None\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching data for {ticker}: {e}\")\n",
        "        return None\n",
        "\n",
        "def compute_features(stock_data):\n",
        "    numeric_cols = ['Open', 'High', 'Low', 'Close']\n",
        "    stock_data[numeric_cols] = stock_data[numeric_cols].astype(float, errors='ignore')\n",
        "\n",
        "    for window in [5, 10, 15, 20, 25, 30]:\n",
        "        stock_data[f'MA_{window}'] = stock_data['Close'].rolling(window=window).mean()\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    stock_data['Normalised Close'] = scaler.fit_transform(stock_data[['Close']])\n",
        "\n",
        "    stock_data['Return Ratio'] = (stock_data['Close'] - stock_data['Close'].shift(1)) / stock_data['Close'].shift(1)\n",
        "    stock_data['Percentage Change Open'] = stock_data['Open'] / stock_data['Close'] - 1\n",
        "    stock_data['Percentage Change High'] = stock_data['High'] / stock_data['Close'] - 1\n",
        "    stock_data['Percentage Change Low'] = stock_data['Low'] / stock_data['Close'] - 1\n",
        "\n",
        "    return stock_data\n",
        "\n",
        "def align_with_reference(stock_data, reference_dates):\n",
        "    start_ref, end_ref = reference_dates\n",
        "    stock_dates = stock_data.index\n",
        "\n",
        "    if stock_dates.min() > start_ref or stock_dates.max() < end_ref:\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def main():\n",
        "    start_date, end_date = \"2022-01-10\", \"2025-01-10\"\n",
        "    companies, sector, sector_encoded = load_company_data(csv_filename)\n",
        "    os.makedirs('Data_is_here', exist_ok=True)\n",
        "\n",
        "    reference_data = fetch_stock_data(\"ONGC\", start_date, end_date)\n",
        "    if reference_data is None:\n",
        "        print(\"Failed to fetch reference data.\")\n",
        "        return\n",
        "    reference_dates = (reference_data.index.min(), reference_data.index.max())\n",
        "\n",
        "    missing = []\n",
        "    for ticker in companies:\n",
        "        stock_data = fetch_stock_data(ticker, start_date, end_date)\n",
        "        if stock_data is not None:\n",
        "            if not align_with_reference(stock_data, reference_dates):\n",
        "                print(f\"Skipping {ticker} due to insufficient data.\")\n",
        "                missing.append(ticker)\n",
        "                continue\n",
        "\n",
        "            stock_data = compute_features(stock_data)\n",
        "\n",
        "            idx = companies.index(ticker) if ticker in companies else -1\n",
        "            if idx != -1:\n",
        "                stock_data['Sector'] = sector[idx]\n",
        "                stock_data['Sector Encoded'] = sector_encoded[idx]\n",
        "\n",
        "            stock_data.to_csv(f'Data_is_here/{ticker}_data.csv')\n",
        "            print(f\"Processed and saved data for {ticker}.\")\n",
        "        else:\n",
        "            print(f\"Skipping {ticker} due to missing data.\")\n",
        "            missing.append(ticker)\n",
        "\n",
        "    with open(\"Data_is_here/skipped_stocks.txt\", \"w\") as file:\n",
        "        for line in missing:\n",
        "            file.write(line + \"\\n\")\n",
        "\n",
        "    print(\"All data processing completed.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmNcB0B2JSCv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a2a205e-fc7e-46e7-d1dc-7662e384d8ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed ZFCVINDIA_data.csv\n",
            "Processed CEATLTD_data.csv\n",
            "Processed BRITANNIA_data.csv\n",
            "Processed TRENT_data.csv\n",
            "Processed MANAPPURAM_data.csv\n",
            "Processed ABSLAMC_data.csv\n",
            "Processed CCL_data.csv\n",
            "Processed ECLERX_data.csv\n",
            "Processed LTTS_data.csv\n",
            "Processed DALBHARAT_data.csv\n",
            "Processed SPARC_data.csv\n",
            "Processed VOLTAS_data.csv\n",
            "Processed NESTLEIND_data.csv\n",
            "Processed INDIGO_data.csv\n",
            "Processed KIMS_data.csv\n",
            "Processed CUMMINSIND_data.csv\n",
            "Processed GUJGASLTD_data.csv\n",
            "Processed BAJAJHLDNG_data.csv\n",
            "Processed DBREALTY_data.csv\n",
            "Processed ADANIGREEN_data.csv\n",
            "Processed HINDZINC_data.csv\n",
            "Processed TATACOMM_data.csv\n",
            "Processed BATAINDIA_data.csv\n",
            "Processed MAXHEALTH_data.csv\n",
            "Processed NBCC_data.csv\n",
            "Processed EIDPARRY_data.csv\n",
            "Processed CENTRALBK_data.csv\n",
            "Processed BANDHANBNK_data.csv\n",
            "Processed TRIVENI_data.csv\n",
            "Processed LAURUSLABS_data.csv\n",
            "Processed MUTHOOTFIN_data.csv\n",
            "Processed SCI_data.csv\n",
            "Processed NTPC_data.csv\n",
            "Processed DEEPAKFERT_data.csv\n",
            "Processed TATAMOTORS_data.csv\n",
            "Processed TIINDIA_data.csv\n",
            "Processed SAIL_data.csv\n",
            "Processed CAPLIPOINT_data.csv\n",
            "Processed ASTRAZEN_data.csv\n",
            "Processed RBLBANK_data.csv\n",
            "Processed CHENNPETRO_data.csv\n",
            "Processed PAGEIND_data.csv\n",
            "Processed UTIAMC_data.csv\n",
            "Processed SANOFI_data.csv\n",
            "Processed MPHASIS_data.csv\n",
            "Processed DIVISLAB_data.csv\n",
            "Processed RELIANCE_data.csv\n",
            "Processed BOSCHLTD_data.csv\n",
            "Processed RAILTEL_data.csv\n",
            "Processed AIAENG_data.csv\n",
            "Processed GMDCLTD_data.csv\n",
            "Processed LALPATHLAB_data.csv\n",
            "Processed AUROPHARMA_data.csv\n",
            "Processed PRAJIND_data.csv\n",
            "Processed PEL_data.csv\n",
            "Processed DEVYANI_data.csv\n",
            "Processed ABB_data.csv\n",
            "Processed CANBK_data.csv\n",
            "Processed COROMANDEL_data.csv\n",
            "Processed BLUEDART_data.csv\n",
            "Processed RECLTD_data.csv\n",
            "Processed GSPL_data.csv\n",
            "Processed BANKINDIA_data.csv\n",
            "Processed AAVAS_data.csv\n",
            "Processed HAVELLS_data.csv\n",
            "Processed RCF_data.csv\n",
            "Processed VIJAYA_data.csv\n",
            "Processed TATAPOWER_data.csv\n",
            "Processed HUDCO_data.csv\n",
            "Processed DMART_data.csv\n",
            "Processed SCHAEFFLER_data.csv\n",
            "Processed TATASTEEL_data.csv\n",
            "Processed EICHERMOT_data.csv\n",
            "Processed NIACL_data.csv\n",
            "Processed POLYCAB_data.csv\n",
            "Processed UBL_data.csv\n",
            "Processed MGL_data.csv\n",
            "Processed UNOMINDA_data.csv\n",
            "Processed INFY_data.csv\n",
            "Processed GODREJPROP_data.csv\n",
            "Processed JPPOWER_data.csv\n",
            "Processed JWL_data.csv\n",
            "Processed GODFRYPHLP_data.csv\n",
            "Processed KANSAINER_data.csv\n",
            "Processed METROBRAND_data.csv\n",
            "Processed APOLLOTYRE_data.csv\n",
            "Processed GODREJCP_data.csv\n",
            "Processed HINDCOPPER_data.csv\n",
            "Processed LT_data.csv\n",
            "Processed NEWGEN_data.csv\n",
            "Processed INDIACEM_data.csv\n",
            "Processed LUPIN_data.csv\n",
            "Processed SUPREMEIND_data.csv\n",
            "Processed INOXWIND_data.csv\n",
            "Processed BAJAJ-AUTO_data.csv\n",
            "Processed ESCORTS_data.csv\n",
            "Processed RENUKA_data.csv\n",
            "Processed ULTRACEMCO_data.csv\n",
            "Processed ROUTE_data.csv\n",
            "Processed HCLTECH_data.csv\n",
            "Processed BDL_data.csv\n",
            "Processed VGUARD_data.csv\n",
            "Processed WIPRO_data.csv\n",
            "Processed PNB_data.csv\n",
            "Processed GESHIP_data.csv\n",
            "Processed HONAUT_data.csv\n",
            "Processed CESC_data.csv\n",
            "Processed SUNDARMFIN_data.csv\n",
            "Processed BLUESTARCO_data.csv\n",
            "Processed COLPAL_data.csv\n",
            "Processed TATAELXSI_data.csv\n",
            "Processed BEL_data.csv\n",
            "Processed AUBANK_data.csv\n",
            "Processed CYIENT_data.csv\n",
            "Processed HDFCLIFE_data.csv\n",
            "Processed QUESS_data.csv\n",
            "Processed JUBLFOOD_data.csv\n",
            "Processed TTML_data.csv\n",
            "Processed SUNDRMFAST_data.csv\n",
            "Processed HDFCAMC_data.csv\n",
            "Processed IDFCFIRSTB_data.csv\n",
            "Processed BALAMINES_data.csv\n",
            "Processed BHEL_data.csv\n",
            "Processed DIXON_data.csv\n",
            "Processed ADANIENT_data.csv\n",
            "Processed GRASIM_data.csv\n",
            "Processed INTELLECT_data.csv\n",
            "Processed INDIANB_data.csv\n",
            "Processed HAPPSTMNDS_data.csv\n",
            "Processed VTL_data.csv\n",
            "Processed TANLA_data.csv\n",
            "Processed GAIL_data.csv\n",
            "Processed NETWORK18_data.csv\n",
            "Processed KEI_data.csv\n",
            "Processed 360ONE_data.csv\n",
            "Processed SIEMENS_data.csv\n",
            "Processed ANGELONE_data.csv\n",
            "Processed REDINGTON_data.csv\n",
            "Processed CANFINHOME_data.csv\n",
            "Processed PNBHOUSING_data.csv\n",
            "Processed GICRE_data.csv\n",
            "Processed RAYMOND_data.csv\n",
            "Processed IRCTC_data.csv\n",
            "Processed CHEMPLASTS_data.csv\n",
            "Processed AARTIIND_data.csv\n",
            "Processed MRF_data.csv\n",
            "Processed OFSS_data.csv\n",
            "Processed CGPOWER_data.csv\n",
            "Processed NAVINFLUOR_data.csv\n",
            "Processed LTIM_data.csv\n",
            "Processed JMFINANCIL_data.csv\n",
            "Processed BHARATFORG_data.csv\n",
            "Processed NYKAA_data.csv\n",
            "Processed CHOLAFIN_data.csv\n",
            "Processed GRSE_data.csv\n",
            "Processed OLECTRA_data.csv\n",
            "Processed MOTILALOFS_data.csv\n",
            "Processed HAL_data.csv\n",
            "Processed KIRLOSBROS_data.csv\n",
            "Processed ZENSARTECH_data.csv\n",
            "Processed ASHOKLEY_data.csv\n",
            "Processed CENTURYPLY_data.csv\n",
            "Processed THERMAX_data.csv\n",
            "Processed SWSOLAR_data.csv\n",
            "Processed RAJESHEXPO_data.csv\n",
            "Processed RITES_data.csv\n",
            "Processed INDUSINDBK_data.csv\n",
            "Processed IDBI_data.csv\n",
            "Processed ELGIEQUIP_data.csv\n",
            "Processed GNFC_data.csv\n",
            "Processed MAHSEAMLES_data.csv\n",
            "Processed ADANIPOWER_data.csv\n",
            "Processed SONACOMS_data.csv\n",
            "Processed SUZLON_data.csv\n",
            "Processed APLLTD_data.csv\n",
            "Processed IGL_data.csv\n",
            "Processed TATACONSUM_data.csv\n",
            "Processed ITC_data.csv\n",
            "Processed BALKRISIND_data.csv\n",
            "Processed TRITURBINE_data.csv\n",
            "Processed KPRMILL_data.csv\n",
            "Processed CUB_data.csv\n",
            "Processed EIHOTEL_data.csv\n",
            "Processed CHOLAHLDNG_data.csv\n",
            "Processed JUBLPHARMA_data.csv\n",
            "Processed DEEPAKNTR_data.csv\n",
            "Processed POLICYBZR_data.csv\n",
            "Processed BHARTIARTL_data.csv\n",
            "Processed TITAGARH_data.csv\n",
            "Processed PAYTM_data.csv\n",
            "Processed APOLLOHOSP_data.csv\n",
            "Processed VBL_data.csv\n",
            "Processed KIRLOSENG_data.csv\n",
            "Processed DRREDDY_data.csv\n",
            "Processed JYOTHYLAB_data.csv\n",
            "Processed MAPMYINDIA_data.csv\n",
            "Processed JINDALSAW_data.csv\n",
            "Processed ACC_data.csv\n",
            "Processed SHRIRAMFIN_data.csv\n",
            "Processed GLAND_data.csv\n",
            "Processed WELCORP_data.csv\n",
            "Processed OBEROIRLTY_data.csv\n",
            "Processed TATAINVEST_data.csv\n",
            "Processed KNRCON_data.csv\n",
            "Processed SRF_data.csv\n",
            "Processed ONGC_data.csv\n",
            "Processed COCHINSHIP_data.csv\n",
            "Processed GLENMARK_data.csv\n",
            "Processed KPITTECH_data.csv\n",
            "Processed RHIM_data.csv\n",
            "Processed ZOMATO_data.csv\n",
            "Processed TATACHEM_data.csv\n",
            "Processed EMAMILTD_data.csv\n",
            "Processed IOC_data.csv\n",
            "Processed RADICO_data.csv\n",
            "Processed BAJAJFINSV_data.csv\n",
            "Processed GILLETTE_data.csv\n",
            "Processed RKFORGE_data.csv\n",
            "Processed SOBHA_data.csv\n",
            "Processed VEDL_data.csv\n",
            "Processed TCS_data.csv\n",
            "Processed JBMA_data.csv\n",
            "Processed VINATIORGA_data.csv\n",
            "Processed PETRONET_data.csv\n",
            "Processed HINDUNILVR_data.csv\n",
            "Processed AMBER_data.csv\n",
            "Processed ELECON_data.csv\n",
            "Processed PERSISTENT_data.csv\n",
            "Processed MASTEK_data.csv\n",
            "Processed SUVENPHAR_data.csv\n",
            "Processed BIRLACORPN_data.csv\n",
            "Processed MAHLIFE_data.csv\n",
            "Processed NLCINDIA_data.csv\n",
            "Processed BLS_data.csv\n",
            "Processed AXISBANK_data.csv\n",
            "Processed TECHNOE_data.csv\n",
            "Processed CIEINDIA_data.csv\n",
            "Processed BSE_data.csv\n",
            "Processed SAREGAMA_data.csv\n",
            "Processed MAHABANK_data.csv\n",
            "Processed BAYERCROP_data.csv\n",
            "Processed FINEORG_data.csv\n",
            "Processed APTUS_data.csv\n",
            "Processed NUVOCO_data.csv\n",
            "Processed MARICO_data.csv\n",
            "Processed EQUITASBNK_data.csv\n",
            "Processed GRINFRA_data.csv\n",
            "Processed NH_data.csv\n",
            "Processed RVNL_data.csv\n",
            "Processed FACT_data.csv\n",
            "Processed GAEL_data.csv\n",
            "Processed ICICIGI_data.csv\n",
            "Processed BPCL_data.csv\n",
            "Processed SONATSOFTW_data.csv\n",
            "Processed KSB_data.csv\n",
            "Processed COALINDIA_data.csv\n",
            "Processed DABUR_data.csv\n",
            "Processed HINDPETRO_data.csv\n",
            "Processed FSL_data.csv\n",
            "Processed GRANULES_data.csv\n",
            "Processed ABCAPITAL_data.csv\n",
            "Processed STARHEALTH_data.csv\n",
            "Processed AJANTPHARM_data.csv\n",
            "Processed PCBL_data.csv\n",
            "Processed VARROC_data.csv\n",
            "Processed FINCABLES_data.csv\n",
            "Processed NAM-INDIA_data.csv\n",
            "Processed WESTLIFE_data.csv\n",
            "Processed BERGEPAINT_data.csv\n",
            "Processed JUBLINGREA_data.csv\n",
            "Processed M&M_data.csv\n",
            "Processed FEDERALBNK_data.csv\n",
            "Processed IPCALAB_data.csv\n",
            "Processed LODHA_data.csv\n",
            "Processed TECHM_data.csv\n",
            "Processed APARINDS_data.csv\n",
            "Processed CROMPTON_data.csv\n",
            "Processed PRESTIGE_data.csv\n",
            "Processed HEG_data.csv\n",
            "Processed JINDALSTEL_data.csv\n",
            "Processed AFFLE_data.csv\n",
            "Processed PFC_data.csv\n",
            "Processed TITAN_data.csv\n",
            "Processed TRIDENT_data.csv\n",
            "Processed HOMEFIRST_data.csv\n",
            "Processed CARBORUNIV_data.csv\n",
            "Processed MINDACORP_data.csv\n",
            "Processed CERA_data.csv\n",
            "Processed UPL_data.csv\n",
            "Processed SJVN_data.csv\n",
            "Processed ERIS_data.csv\n",
            "Processed PIIND_data.csv\n",
            "Processed GRAPHITE_data.csv\n",
            "Processed USHAMART_data.csv\n",
            "Processed ABBOTINDIA_data.csv\n",
            "Processed JKLAKSHMI_data.csv\n",
            "Processed SBIN_data.csv\n",
            "Processed SUNTV_data.csv\n",
            "Processed SOLARINDS_data.csv\n",
            "Processed TIMKEN_data.csv\n",
            "Processed PGHH_data.csv\n",
            "Processed SHREECEM_data.csv\n",
            "Processed VIPIND_data.csv\n",
            "Processed SUNPHARMA_data.csv\n",
            "Processed ZEEL_data.csv\n",
            "Processed MAZDOCK_data.csv\n",
            "Processed UNIONBANK_data.csv\n",
            "Processed EXIDEIND_data.csv\n",
            "Processed CRISIL_data.csv\n",
            "Processed CDSL_data.csv\n",
            "Processed ALKEM_data.csv\n",
            "Processed HFCL_data.csv\n",
            "Processed NHPC_data.csv\n",
            "Processed APLAPOLLO_data.csv\n",
            "Processed SYNGENE_data.csv\n",
            "Processed KEC_data.csv\n",
            "Processed TVSMOTOR_data.csv\n",
            "Processed GPPL_data.csv\n",
            "Processed NAUKRI_data.csv\n",
            "Processed NATCOPHARM_data.csv\n",
            "Processed TEJASNET_data.csv\n",
            "Processed CASTROLIND_data.csv\n",
            "Processed ALOKINDS_data.csv\n",
            "Processed MRPL_data.csv\n",
            "Processed SUMICHEM_data.csv\n",
            "Processed TORNTPHARM_data.csv\n",
            "Processed ICICIBANK_data.csv\n",
            "Processed BALRAMCHIN_data.csv\n",
            "Processed YESBANK_data.csv\n",
            "Processed ASIANPAINT_data.csv\n",
            "Processed ASAHIINDIA_data.csv\n",
            "Processed BEML_data.csv\n",
            "Processed J&KBANK_data.csv\n",
            "Processed ITI_data.csv\n",
            "Processed ASTERDM_data.csv\n",
            "Processed RTNINDIA_data.csv\n",
            "Processed UJJIVANSFB_data.csv\n",
            "Processed POLYMED_data.csv\n",
            "Processed MCX_data.csv\n",
            "Processed KAJARIACER_data.csv\n",
            "Processed AMBUJACEM_data.csv\n",
            "Processed GPIL_data.csv\n",
            "Processed SKFINDIA_data.csv\n",
            "Processed SAPPHIRE_data.csv\n",
            "Processed NCC_data.csv\n",
            "Processed DATAPATTNS_data.csv\n",
            "Processed JKTYRE_data.csv\n",
            "Processed NATIONALUM_data.csv\n",
            "Processed ATUL_data.csv\n",
            "Processed FINPIPE_data.csv\n",
            "Processed BIOCON_data.csv\n",
            "Processed GVT&D_data.csv\n",
            "Processed RAMCOCEM_data.csv\n",
            "Processed UCOBANK_data.csv\n",
            "Processed PIDILITIND_data.csv\n",
            "Processed OIL_data.csv\n",
            "Processed ENGINERSIN_data.csv\n",
            "Processed RATNAMANI_data.csv\n",
            "Processed DLF_data.csv\n",
            "Processed MARUTI_data.csv\n",
            "Processed SBICARD_data.csv\n",
            "Processed AVANTIFEED_data.csv\n",
            "Processed LINDEINDIA_data.csv\n",
            "Processed ALKYLAMINE_data.csv\n",
            "Processed BBTC_data.csv\n",
            "Processed GSFC_data.csv\n",
            "Processed POWERINDIA_data.csv\n",
            "Processed IEX_data.csv\n",
            "Processed CHAMBLFERT_data.csv\n",
            "Processed PFIZER_data.csv\n",
            "Processed INDIAMART_data.csv\n",
            "Processed HEROMOTOCO_data.csv\n",
            "Processed BANKBARODA_data.csv\n",
            "Processed GLAXO_data.csv\n",
            "Processed METROPOLIS_data.csv\n",
            "Processed CRAFTSMAN_data.csv\n",
            "Processed KARURVYSYA_data.csv\n",
            "Processed IOB_data.csv\n",
            "Processed CGCL_data.csv\n",
            "Processed INDUSTOWER_data.csv\n",
            "Processed JUSTDIAL_data.csv\n",
            "Processed ZYDUSLIFE_data.csv\n",
            "Processed LICHSGFIN_data.csv\n",
            "Processed JBCHEPHARM_data.csv\n",
            "Processed HDFCBANK_data.csv\n",
            "Processed CREDITACC_data.csv\n",
            "Processed ENDURANCE_data.csv\n",
            "Processed FLUOROCHEM_data.csv\n",
            "Processed JSL_data.csv\n",
            "Processed CLEAN_data.csv\n",
            "Processed KOTAKBANK_data.csv\n",
            "Processed SAMMAANCAP_data.csv\n",
            "Processed ABFRL_data.csv\n",
            "Processed CHALET_data.csv\n",
            "Processed GRINDWELL_data.csv\n",
            "Processed MMTC_data.csv\n",
            "Processed JSWSTEEL_data.csv\n",
            "Processed POWERGRID_data.csv\n",
            "Processed TORNTPOWER_data.csv\n",
            "Processed CAMS_data.csv\n",
            "Processed IRB_data.csv\n",
            "Processed COFORGE_data.csv\n",
            "Processed IRFC_data.csv\n",
            "Processed HINDALCO_data.csv\n",
            "Processed ADANIPORTS_data.csv\n",
            "Processed BSOFT_data.csv\n",
            "Processed FORTIS_data.csv\n",
            "Processed M&MFIN_data.csv\n",
            "Processed WELSPUNLIV_data.csv\n",
            "Processed LTF_data.csv\n",
            "Processed PNCINFRA_data.csv\n",
            "Processed ACE_data.csv\n",
            "Processed IDEA_data.csv\n",
            "Processed HSCL_data.csv\n",
            "Processed IFCI_data.csv\n",
            "Processed ANANTRAJ_data.csv\n",
            "Processed KALYANKJIL_data.csv\n",
            "Processed SHYAMMETL_data.csv\n",
            "Processed PATANJALI_data.csv\n",
            "Processed MOTHERSON_data.csv\n",
            "Processed GMRAIRPORT_data.csv\n",
            "Processed SCHNEIDER_data.csv\n",
            "Processed EASEMYTRIP_data.csv\n",
            "Processed SBILIFE_data.csv\n",
            "Processed WHIRLPOOL_data.csv\n",
            "Processed NMDC_data.csv\n",
            "Processed ANANDRATHI_data.csv\n",
            "Processed LEMONTREE_data.csv\n",
            "Processed IRCON_data.csv\n",
            "Processed CIPLA_data.csv\n",
            "Processed ICICIPRULI_data.csv\n",
            "Processed MFSL_data.csv\n",
            "Processed JSWENERGY_data.csv\n",
            "Processed BASF_data.csv\n",
            "Processed CONCOR_data.csv\n",
            "Processed INDHOTEL_data.csv\n",
            "Processed IIFL_data.csv\n",
            "Processed ATGL_data.csv\n",
            "Processed POONAWALLA_data.csv\n",
            "Processed LATENTVIEW_data.csv\n",
            "Processed JKCEMENT_data.csv\n",
            "Processed BAJFINANCE_data.csv\n",
            "Processed GODREJAGRO_data.csv\n",
            "Processed ASTRAL_data.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "file_path = \"/content/Data_is_here\"\n",
        "\n",
        "# Ensure the directory exists\n",
        "if not os.path.exists(file_path):\n",
        "    print(\"Error: Directory does not exist.\")\n",
        "    exit()\n",
        "\n",
        "# Get list of CSV files\n",
        "files = [f for f in os.listdir(file_path) if f.endswith(\".csv\")]\n",
        "\n",
        "for file in files:\n",
        "    file_full_path = os.path.join(file_path, file)\n",
        "\n",
        "    df = pd.read_csv(file_full_path)\n",
        "\n",
        "    # Ensure 'Return Ratio' exists in DataFrame\n",
        "    if 'Return Ratio' not in df.columns:\n",
        "        print(f\"Skipping {file}: 'Return Ratio' column not found.\")\n",
        "        continue\n",
        "\n",
        "    # Create the Stock_Movement_Label column\n",
        "    df['Stock_Movement_Label'] = (df['Return Ratio'].shift(-5) > 0).astype(int)\n",
        "\n",
        "    # Save the modified file\n",
        "    df.to_csv(file_full_path, index=False)\n",
        "    print(f\"Processed {file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ctvx7dMaM0OC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17a6661d-f404-4bd7-8eeb-016082519f18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file: ZFCVINDIA_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed ZFCVINDIA_data.csv successfully.\n",
            "Processing file: CEATLTD_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed CEATLTD_data.csv successfully.\n",
            "Processing file: BRITANNIA_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed BRITANNIA_data.csv successfully.\n",
            "Processing file: TRENT_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed TRENT_data.csv successfully.\n",
            "Processing file: MANAPPURAM_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed MANAPPURAM_data.csv successfully.\n",
            "Processing file: ABSLAMC_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed ABSLAMC_data.csv successfully.\n",
            "Processing file: CCL_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed CCL_data.csv successfully.\n",
            "Processing file: ECLERX_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed ECLERX_data.csv successfully.\n",
            "Processing file: LTTS_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed LTTS_data.csv successfully.\n",
            "Processing file: DALBHARAT_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed DALBHARAT_data.csv successfully.\n",
            "Processing file: SPARC_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed SPARC_data.csv successfully.\n",
            "Processing file: VOLTAS_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed VOLTAS_data.csv successfully.\n",
            "Processing file: NESTLEIND_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed NESTLEIND_data.csv successfully.\n",
            "Processing file: INDIGO_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed INDIGO_data.csv successfully.\n",
            "Processing file: KIMS_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed KIMS_data.csv successfully.\n",
            "Processing file: CUMMINSIND_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed CUMMINSIND_data.csv successfully.\n",
            "Processing file: GUJGASLTD_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed GUJGASLTD_data.csv successfully.\n",
            "Processing file: BAJAJHLDNG_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed BAJAJHLDNG_data.csv successfully.\n",
            "Processing file: DBREALTY_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed DBREALTY_data.csv successfully.\n",
            "Processing file: ADANIGREEN_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed ADANIGREEN_data.csv successfully.\n",
            "Processing file: HINDZINC_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed HINDZINC_data.csv successfully.\n",
            "Processing file: TATACOMM_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed TATACOMM_data.csv successfully.\n",
            "Processing file: BATAINDIA_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed BATAINDIA_data.csv successfully.\n",
            "Processing file: MAXHEALTH_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed MAXHEALTH_data.csv successfully.\n",
            "Processing file: NBCC_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed NBCC_data.csv successfully.\n",
            "Processing file: EIDPARRY_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed EIDPARRY_data.csv successfully.\n",
            "Processing file: CENTRALBK_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed CENTRALBK_data.csv successfully.\n",
            "Processing file: BANDHANBNK_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed BANDHANBNK_data.csv successfully.\n",
            "Processing file: TRIVENI_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed TRIVENI_data.csv successfully.\n",
            "Processing file: LAURUSLABS_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed LAURUSLABS_data.csv successfully.\n",
            "Processing file: MUTHOOTFIN_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed MUTHOOTFIN_data.csv successfully.\n",
            "Processing file: SCI_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed SCI_data.csv successfully.\n",
            "Processing file: NTPC_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed NTPC_data.csv successfully.\n",
            "Processing file: DEEPAKFERT_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed DEEPAKFERT_data.csv successfully.\n",
            "Processing file: TATAMOTORS_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed TATAMOTORS_data.csv successfully.\n",
            "Processing file: TIINDIA_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed TIINDIA_data.csv successfully.\n",
            "Processing file: SAIL_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed SAIL_data.csv successfully.\n",
            "Processing file: CAPLIPOINT_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed CAPLIPOINT_data.csv successfully.\n",
            "Processing file: ASTRAZEN_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed ASTRAZEN_data.csv successfully.\n",
            "Processing file: RBLBANK_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed RBLBANK_data.csv successfully.\n",
            "Processing file: CHENNPETRO_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed CHENNPETRO_data.csv successfully.\n",
            "Processing file: PAGEIND_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed PAGEIND_data.csv successfully.\n",
            "Processing file: UTIAMC_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed UTIAMC_data.csv successfully.\n",
            "Processing file: SANOFI_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed SANOFI_data.csv successfully.\n",
            "Processing file: MPHASIS_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed MPHASIS_data.csv successfully.\n",
            "Processing file: DIVISLAB_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed DIVISLAB_data.csv successfully.\n",
            "Processing file: RELIANCE_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed RELIANCE_data.csv successfully.\n",
            "Processing file: BOSCHLTD_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed BOSCHLTD_data.csv successfully.\n",
            "Processing file: RAILTEL_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed RAILTEL_data.csv successfully.\n",
            "Processing file: AIAENG_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed AIAENG_data.csv successfully.\n",
            "Processing file: GMDCLTD_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed GMDCLTD_data.csv successfully.\n",
            "Processing file: LALPATHLAB_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed LALPATHLAB_data.csv successfully.\n",
            "Processing file: AUROPHARMA_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed AUROPHARMA_data.csv successfully.\n",
            "Processing file: PRAJIND_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed PRAJIND_data.csv successfully.\n",
            "Processing file: PEL_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed PEL_data.csv successfully.\n",
            "Processing file: DEVYANI_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed DEVYANI_data.csv successfully.\n",
            "Processing file: ABB_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed ABB_data.csv successfully.\n",
            "Processing file: CANBK_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed CANBK_data.csv successfully.\n",
            "Processing file: COROMANDEL_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed COROMANDEL_data.csv successfully.\n",
            "Processing file: BLUEDART_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed BLUEDART_data.csv successfully.\n",
            "Processing file: RECLTD_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed RECLTD_data.csv successfully.\n",
            "Processing file: GSPL_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed GSPL_data.csv successfully.\n",
            "Processing file: BANKINDIA_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed BANKINDIA_data.csv successfully.\n",
            "Processing file: AAVAS_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed AAVAS_data.csv successfully.\n",
            "Processing file: HAVELLS_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed HAVELLS_data.csv successfully.\n",
            "Processing file: RCF_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed RCF_data.csv successfully.\n",
            "Processing file: VIJAYA_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed VIJAYA_data.csv successfully.\n",
            "Processing file: TATAPOWER_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed TATAPOWER_data.csv successfully.\n",
            "Processing file: HUDCO_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed HUDCO_data.csv successfully.\n",
            "Processing file: DMART_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed DMART_data.csv successfully.\n",
            "Processing file: SCHAEFFLER_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed SCHAEFFLER_data.csv successfully.\n",
            "Processing file: TATASTEEL_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed TATASTEEL_data.csv successfully.\n",
            "Processing file: EICHERMOT_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed EICHERMOT_data.csv successfully.\n",
            "Processing file: NIACL_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed NIACL_data.csv successfully.\n",
            "Processing file: POLYCAB_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed POLYCAB_data.csv successfully.\n",
            "Processing file: UBL_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed UBL_data.csv successfully.\n",
            "Processing file: MGL_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed MGL_data.csv successfully.\n",
            "Processing file: UNOMINDA_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed UNOMINDA_data.csv successfully.\n",
            "Processing file: INFY_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed INFY_data.csv successfully.\n",
            "Processing file: GODREJPROP_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed GODREJPROP_data.csv successfully.\n",
            "Processing file: JPPOWER_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed JPPOWER_data.csv successfully.\n",
            "Processing file: JWL_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed JWL_data.csv successfully.\n",
            "Processing file: GODFRYPHLP_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed GODFRYPHLP_data.csv successfully.\n",
            "Processing file: KANSAINER_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed KANSAINER_data.csv successfully.\n",
            "Processing file: METROBRAND_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed METROBRAND_data.csv successfully.\n",
            "Processing file: APOLLOTYRE_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed APOLLOTYRE_data.csv successfully.\n",
            "Processing file: GODREJCP_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed GODREJCP_data.csv successfully.\n",
            "Processing file: HINDCOPPER_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed HINDCOPPER_data.csv successfully.\n",
            "Processing file: LT_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed LT_data.csv successfully.\n",
            "Processing file: NEWGEN_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed NEWGEN_data.csv successfully.\n",
            "Processing file: INDIACEM_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed INDIACEM_data.csv successfully.\n",
            "Processing file: LUPIN_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed LUPIN_data.csv successfully.\n",
            "Processing file: SUPREMEIND_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed SUPREMEIND_data.csv successfully.\n",
            "Processing file: INOXWIND_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed INOXWIND_data.csv successfully.\n",
            "Processing file: BAJAJ-AUTO_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed BAJAJ-AUTO_data.csv successfully.\n",
            "Processing file: ESCORTS_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed ESCORTS_data.csv successfully.\n",
            "Processing file: RENUKA_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed RENUKA_data.csv successfully.\n",
            "Processing file: ULTRACEMCO_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed ULTRACEMCO_data.csv successfully.\n",
            "Processing file: ROUTE_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed ROUTE_data.csv successfully.\n",
            "Processing file: HCLTECH_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed HCLTECH_data.csv successfully.\n",
            "Processing file: BDL_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed BDL_data.csv successfully.\n",
            "Processing file: VGUARD_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed VGUARD_data.csv successfully.\n",
            "Processing file: WIPRO_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed WIPRO_data.csv successfully.\n",
            "Processing file: PNB_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed PNB_data.csv successfully.\n",
            "Processing file: GESHIP_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed GESHIP_data.csv successfully.\n",
            "Processing file: HONAUT_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed HONAUT_data.csv successfully.\n",
            "Processing file: CESC_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed CESC_data.csv successfully.\n",
            "Processing file: SUNDARMFIN_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed SUNDARMFIN_data.csv successfully.\n",
            "Processing file: BLUESTARCO_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed BLUESTARCO_data.csv successfully.\n",
            "Processing file: COLPAL_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed COLPAL_data.csv successfully.\n",
            "Processing file: TATAELXSI_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed TATAELXSI_data.csv successfully.\n",
            "Processing file: BEL_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed BEL_data.csv successfully.\n",
            "Processing file: AUBANK_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed AUBANK_data.csv successfully.\n",
            "Processing file: CYIENT_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed CYIENT_data.csv successfully.\n",
            "Processing file: HDFCLIFE_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed HDFCLIFE_data.csv successfully.\n",
            "Processing file: QUESS_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed QUESS_data.csv successfully.\n",
            "Processing file: JUBLFOOD_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed JUBLFOOD_data.csv successfully.\n",
            "Processing file: TTML_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed TTML_data.csv successfully.\n",
            "Processing file: SUNDRMFAST_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed SUNDRMFAST_data.csv successfully.\n",
            "Processing file: HDFCAMC_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed HDFCAMC_data.csv successfully.\n",
            "Processing file: IDFCFIRSTB_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed IDFCFIRSTB_data.csv successfully.\n",
            "Processing file: BALAMINES_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed BALAMINES_data.csv successfully.\n",
            "Processing file: BHEL_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed BHEL_data.csv successfully.\n",
            "Processing file: DIXON_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed DIXON_data.csv successfully.\n",
            "Processing file: ADANIENT_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed ADANIENT_data.csv successfully.\n",
            "Processing file: GRASIM_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed GRASIM_data.csv successfully.\n",
            "Processing file: INTELLECT_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed INTELLECT_data.csv successfully.\n",
            "Processing file: INDIANB_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed INDIANB_data.csv successfully.\n",
            "Processing file: HAPPSTMNDS_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed HAPPSTMNDS_data.csv successfully.\n",
            "Processing file: VTL_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed VTL_data.csv successfully.\n",
            "Processing file: TANLA_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed TANLA_data.csv successfully.\n",
            "Processing file: GAIL_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed GAIL_data.csv successfully.\n",
            "Processing file: NETWORK18_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed NETWORK18_data.csv successfully.\n",
            "Processing file: KEI_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed KEI_data.csv successfully.\n",
            "Processing file: 360ONE_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed 360ONE_data.csv successfully.\n",
            "Processing file: SIEMENS_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed SIEMENS_data.csv successfully.\n",
            "Processing file: ANGELONE_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed ANGELONE_data.csv successfully.\n",
            "Processing file: REDINGTON_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed REDINGTON_data.csv successfully.\n",
            "Processing file: CANFINHOME_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed CANFINHOME_data.csv successfully.\n",
            "Processing file: PNBHOUSING_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed PNBHOUSING_data.csv successfully.\n",
            "Processing file: GICRE_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed GICRE_data.csv successfully.\n",
            "Processing file: RAYMOND_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed RAYMOND_data.csv successfully.\n",
            "Processing file: IRCTC_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed IRCTC_data.csv successfully.\n",
            "Processing file: CHEMPLASTS_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed CHEMPLASTS_data.csv successfully.\n",
            "Processing file: AARTIIND_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed AARTIIND_data.csv successfully.\n",
            "Processing file: MRF_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed MRF_data.csv successfully.\n",
            "Processing file: OFSS_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed OFSS_data.csv successfully.\n",
            "Processing file: CGPOWER_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed CGPOWER_data.csv successfully.\n",
            "Processing file: NAVINFLUOR_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed NAVINFLUOR_data.csv successfully.\n",
            "Processing file: LTIM_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed LTIM_data.csv successfully.\n",
            "Processing file: JMFINANCIL_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed JMFINANCIL_data.csv successfully.\n",
            "Processing file: BHARATFORG_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed BHARATFORG_data.csv successfully.\n",
            "Processing file: NYKAA_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed NYKAA_data.csv successfully.\n",
            "Processing file: CHOLAFIN_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed CHOLAFIN_data.csv successfully.\n",
            "Processing file: GRSE_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed GRSE_data.csv successfully.\n",
            "Processing file: OLECTRA_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed OLECTRA_data.csv successfully.\n",
            "Processing file: MOTILALOFS_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed MOTILALOFS_data.csv successfully.\n",
            "Processing file: HAL_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed HAL_data.csv successfully.\n",
            "Processing file: KIRLOSBROS_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed KIRLOSBROS_data.csv successfully.\n",
            "Processing file: ZENSARTECH_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed ZENSARTECH_data.csv successfully.\n",
            "Processing file: ASHOKLEY_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed ASHOKLEY_data.csv successfully.\n",
            "Processing file: CENTURYPLY_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed CENTURYPLY_data.csv successfully.\n",
            "Processing file: THERMAX_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed THERMAX_data.csv successfully.\n",
            "Processing file: SWSOLAR_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed SWSOLAR_data.csv successfully.\n",
            "Processing file: RAJESHEXPO_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed RAJESHEXPO_data.csv successfully.\n",
            "Processing file: RITES_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed RITES_data.csv successfully.\n",
            "Processing file: INDUSINDBK_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed INDUSINDBK_data.csv successfully.\n",
            "Processing file: IDBI_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed IDBI_data.csv successfully.\n",
            "Processing file: ELGIEQUIP_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed ELGIEQUIP_data.csv successfully.\n",
            "Processing file: GNFC_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed GNFC_data.csv successfully.\n",
            "Processing file: MAHSEAMLES_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed MAHSEAMLES_data.csv successfully.\n",
            "Processing file: ADANIPOWER_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed ADANIPOWER_data.csv successfully.\n",
            "Processing file: SONACOMS_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed SONACOMS_data.csv successfully.\n",
            "Processing file: SUZLON_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed SUZLON_data.csv successfully.\n",
            "Processing file: APLLTD_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed APLLTD_data.csv successfully.\n",
            "Processing file: IGL_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed IGL_data.csv successfully.\n",
            "Processing file: TATACONSUM_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed TATACONSUM_data.csv successfully.\n",
            "Processing file: ITC_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed ITC_data.csv successfully.\n",
            "Processing file: BALKRISIND_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed BALKRISIND_data.csv successfully.\n",
            "Processing file: TRITURBINE_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed TRITURBINE_data.csv successfully.\n",
            "Processing file: KPRMILL_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed KPRMILL_data.csv successfully.\n",
            "Processing file: CUB_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed CUB_data.csv successfully.\n",
            "Processing file: EIHOTEL_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed EIHOTEL_data.csv successfully.\n",
            "Processing file: CHOLAHLDNG_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed CHOLAHLDNG_data.csv successfully.\n",
            "Processing file: JUBLPHARMA_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed JUBLPHARMA_data.csv successfully.\n",
            "Processing file: DEEPAKNTR_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed DEEPAKNTR_data.csv successfully.\n",
            "Processing file: POLICYBZR_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed POLICYBZR_data.csv successfully.\n",
            "Processing file: BHARTIARTL_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed BHARTIARTL_data.csv successfully.\n",
            "Processing file: TITAGARH_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed TITAGARH_data.csv successfully.\n",
            "Processing file: PAYTM_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed PAYTM_data.csv successfully.\n",
            "Processing file: APOLLOHOSP_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed APOLLOHOSP_data.csv successfully.\n",
            "Processing file: VBL_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed VBL_data.csv successfully.\n",
            "Processing file: KIRLOSENG_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed KIRLOSENG_data.csv successfully.\n",
            "Processing file: DRREDDY_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed DRREDDY_data.csv successfully.\n",
            "Processing file: JYOTHYLAB_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed JYOTHYLAB_data.csv successfully.\n",
            "Processing file: MAPMYINDIA_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed MAPMYINDIA_data.csv successfully.\n",
            "Processing file: JINDALSAW_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed JINDALSAW_data.csv successfully.\n",
            "Processing file: ACC_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed ACC_data.csv successfully.\n",
            "Processing file: SHRIRAMFIN_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed SHRIRAMFIN_data.csv successfully.\n",
            "Processing file: GLAND_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed GLAND_data.csv successfully.\n",
            "Processing file: WELCORP_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed WELCORP_data.csv successfully.\n",
            "Processing file: OBEROIRLTY_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed OBEROIRLTY_data.csv successfully.\n",
            "Processing file: TATAINVEST_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed TATAINVEST_data.csv successfully.\n",
            "Processing file: KNRCON_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed KNRCON_data.csv successfully.\n",
            "Processing file: SRF_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed SRF_data.csv successfully.\n",
            "Processing file: ONGC_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed ONGC_data.csv successfully.\n",
            "Processing file: COCHINSHIP_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed COCHINSHIP_data.csv successfully.\n",
            "Processing file: GLENMARK_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed GLENMARK_data.csv successfully.\n",
            "Processing file: KPITTECH_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed KPITTECH_data.csv successfully.\n",
            "Processing file: RHIM_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed RHIM_data.csv successfully.\n",
            "Processing file: ZOMATO_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed ZOMATO_data.csv successfully.\n",
            "Processing file: TATACHEM_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed TATACHEM_data.csv successfully.\n",
            "Processing file: EMAMILTD_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed EMAMILTD_data.csv successfully.\n",
            "Processing file: IOC_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed IOC_data.csv successfully.\n",
            "Processing file: RADICO_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed RADICO_data.csv successfully.\n",
            "Processing file: BAJAJFINSV_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed BAJAJFINSV_data.csv successfully.\n",
            "Processing file: GILLETTE_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed GILLETTE_data.csv successfully.\n",
            "Processing file: RKFORGE_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed RKFORGE_data.csv successfully.\n",
            "Processing file: SOBHA_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed SOBHA_data.csv successfully.\n",
            "Processing file: VEDL_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed VEDL_data.csv successfully.\n",
            "Processing file: TCS_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed TCS_data.csv successfully.\n",
            "Processing file: JBMA_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed JBMA_data.csv successfully.\n",
            "Processing file: VINATIORGA_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed VINATIORGA_data.csv successfully.\n",
            "Processing file: PETRONET_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed PETRONET_data.csv successfully.\n",
            "Processing file: HINDUNILVR_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed HINDUNILVR_data.csv successfully.\n",
            "Processing file: AMBER_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed AMBER_data.csv successfully.\n",
            "Processing file: ELECON_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed ELECON_data.csv successfully.\n",
            "Processing file: PERSISTENT_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed PERSISTENT_data.csv successfully.\n",
            "Processing file: MASTEK_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed MASTEK_data.csv successfully.\n",
            "Processing file: SUVENPHAR_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed SUVENPHAR_data.csv successfully.\n",
            "Processing file: BIRLACORPN_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed BIRLACORPN_data.csv successfully.\n",
            "Processing file: MAHLIFE_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed MAHLIFE_data.csv successfully.\n",
            "Processing file: NLCINDIA_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed NLCINDIA_data.csv successfully.\n",
            "Processing file: BLS_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed BLS_data.csv successfully.\n",
            "Processing file: AXISBANK_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed AXISBANK_data.csv successfully.\n",
            "Processing file: TECHNOE_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed TECHNOE_data.csv successfully.\n",
            "Processing file: CIEINDIA_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed CIEINDIA_data.csv successfully.\n",
            "Processing file: BSE_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed BSE_data.csv successfully.\n",
            "Processing file: SAREGAMA_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed SAREGAMA_data.csv successfully.\n",
            "Processing file: MAHABANK_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed MAHABANK_data.csv successfully.\n",
            "Processing file: BAYERCROP_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed BAYERCROP_data.csv successfully.\n",
            "Processing file: FINEORG_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed FINEORG_data.csv successfully.\n",
            "Processing file: APTUS_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed APTUS_data.csv successfully.\n",
            "Processing file: NUVOCO_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed NUVOCO_data.csv successfully.\n",
            "Processing file: MARICO_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed MARICO_data.csv successfully.\n",
            "Processing file: EQUITASBNK_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed EQUITASBNK_data.csv successfully.\n",
            "Processing file: GRINFRA_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed GRINFRA_data.csv successfully.\n",
            "Processing file: NH_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed NH_data.csv successfully.\n",
            "Processing file: RVNL_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed RVNL_data.csv successfully.\n",
            "Processing file: FACT_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed FACT_data.csv successfully.\n",
            "Processing file: GAEL_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed GAEL_data.csv successfully.\n",
            "Processing file: ICICIGI_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed ICICIGI_data.csv successfully.\n",
            "Processing file: BPCL_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed BPCL_data.csv successfully.\n",
            "Processing file: SONATSOFTW_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed SONATSOFTW_data.csv successfully.\n",
            "Processing file: KSB_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed KSB_data.csv successfully.\n",
            "Processing file: COALINDIA_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed COALINDIA_data.csv successfully.\n",
            "Processing file: DABUR_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed DABUR_data.csv successfully.\n",
            "Processing file: HINDPETRO_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed HINDPETRO_data.csv successfully.\n",
            "Processing file: FSL_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed FSL_data.csv successfully.\n",
            "Processing file: GRANULES_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed GRANULES_data.csv successfully.\n",
            "Processing file: ABCAPITAL_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed ABCAPITAL_data.csv successfully.\n",
            "Processing file: STARHEALTH_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed STARHEALTH_data.csv successfully.\n",
            "Processing file: AJANTPHARM_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed AJANTPHARM_data.csv successfully.\n",
            "Processing file: PCBL_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed PCBL_data.csv successfully.\n",
            "Processing file: VARROC_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed VARROC_data.csv successfully.\n",
            "Processing file: FINCABLES_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed FINCABLES_data.csv successfully.\n",
            "Processing file: NAM-INDIA_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed NAM-INDIA_data.csv successfully.\n",
            "Processing file: WESTLIFE_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed WESTLIFE_data.csv successfully.\n",
            "Processing file: BERGEPAINT_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed BERGEPAINT_data.csv successfully.\n",
            "Processing file: JUBLINGREA_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed JUBLINGREA_data.csv successfully.\n",
            "Processing file: M&M_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed M&M_data.csv successfully.\n",
            "Processing file: FEDERALBNK_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed FEDERALBNK_data.csv successfully.\n",
            "Processing file: IPCALAB_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed IPCALAB_data.csv successfully.\n",
            "Processing file: LODHA_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed LODHA_data.csv successfully.\n",
            "Processing file: TECHM_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed TECHM_data.csv successfully.\n",
            "Processing file: APARINDS_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed APARINDS_data.csv successfully.\n",
            "Processing file: CROMPTON_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed CROMPTON_data.csv successfully.\n",
            "Processing file: PRESTIGE_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed PRESTIGE_data.csv successfully.\n",
            "Processing file: HEG_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed HEG_data.csv successfully.\n",
            "Processing file: JINDALSTEL_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed JINDALSTEL_data.csv successfully.\n",
            "Processing file: AFFLE_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed AFFLE_data.csv successfully.\n",
            "Processing file: PFC_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed PFC_data.csv successfully.\n",
            "Processing file: TITAN_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed TITAN_data.csv successfully.\n",
            "Processing file: TRIDENT_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed TRIDENT_data.csv successfully.\n",
            "Processing file: HOMEFIRST_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed HOMEFIRST_data.csv successfully.\n",
            "Processing file: CARBORUNIV_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed CARBORUNIV_data.csv successfully.\n",
            "Processing file: MINDACORP_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed MINDACORP_data.csv successfully.\n",
            "Processing file: CERA_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed CERA_data.csv successfully.\n",
            "Processing file: UPL_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed UPL_data.csv successfully.\n",
            "Processing file: SJVN_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed SJVN_data.csv successfully.\n",
            "Processing file: ERIS_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed ERIS_data.csv successfully.\n",
            "Processing file: PIIND_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed PIIND_data.csv successfully.\n",
            "Processing file: GRAPHITE_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed GRAPHITE_data.csv successfully.\n",
            "Processing file: USHAMART_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed USHAMART_data.csv successfully.\n",
            "Processing file: ABBOTINDIA_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed ABBOTINDIA_data.csv successfully.\n",
            "Processing file: JKLAKSHMI_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed JKLAKSHMI_data.csv successfully.\n",
            "Processing file: SBIN_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed SBIN_data.csv successfully.\n",
            "Processing file: SUNTV_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed SUNTV_data.csv successfully.\n",
            "Processing file: SOLARINDS_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed SOLARINDS_data.csv successfully.\n",
            "Processing file: TIMKEN_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed TIMKEN_data.csv successfully.\n",
            "Processing file: PGHH_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed PGHH_data.csv successfully.\n",
            "Processing file: SHREECEM_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed SHREECEM_data.csv successfully.\n",
            "Processing file: VIPIND_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed VIPIND_data.csv successfully.\n",
            "Processing file: SUNPHARMA_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed SUNPHARMA_data.csv successfully.\n",
            "Processing file: ZEEL_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed ZEEL_data.csv successfully.\n",
            "Processing file: MAZDOCK_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed MAZDOCK_data.csv successfully.\n",
            "Processing file: UNIONBANK_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed UNIONBANK_data.csv successfully.\n",
            "Processing file: EXIDEIND_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed EXIDEIND_data.csv successfully.\n",
            "Processing file: CRISIL_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed CRISIL_data.csv successfully.\n",
            "Processing file: CDSL_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed CDSL_data.csv successfully.\n",
            "Processing file: ALKEM_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed ALKEM_data.csv successfully.\n",
            "Processing file: HFCL_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed HFCL_data.csv successfully.\n",
            "Processing file: NHPC_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed NHPC_data.csv successfully.\n",
            "Processing file: APLAPOLLO_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed APLAPOLLO_data.csv successfully.\n",
            "Processing file: SYNGENE_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed SYNGENE_data.csv successfully.\n",
            "Processing file: KEC_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed KEC_data.csv successfully.\n",
            "Processing file: TVSMOTOR_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed TVSMOTOR_data.csv successfully.\n",
            "Processing file: GPPL_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed GPPL_data.csv successfully.\n",
            "Processing file: NAUKRI_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed NAUKRI_data.csv successfully.\n",
            "Processing file: NATCOPHARM_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed NATCOPHARM_data.csv successfully.\n",
            "Processing file: TEJASNET_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed TEJASNET_data.csv successfully.\n",
            "Processing file: CASTROLIND_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed CASTROLIND_data.csv successfully.\n",
            "Processing file: ALOKINDS_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed ALOKINDS_data.csv successfully.\n",
            "Processing file: MRPL_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed MRPL_data.csv successfully.\n",
            "Processing file: SUMICHEM_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed SUMICHEM_data.csv successfully.\n",
            "Processing file: TORNTPHARM_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed TORNTPHARM_data.csv successfully.\n",
            "Processing file: ICICIBANK_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed ICICIBANK_data.csv successfully.\n",
            "Processing file: BALRAMCHIN_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed BALRAMCHIN_data.csv successfully.\n",
            "Processing file: YESBANK_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed YESBANK_data.csv successfully.\n",
            "Processing file: ASIANPAINT_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed ASIANPAINT_data.csv successfully.\n",
            "Processing file: ASAHIINDIA_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed ASAHIINDIA_data.csv successfully.\n",
            "Processing file: BEML_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed BEML_data.csv successfully.\n",
            "Processing file: J&KBANK_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed J&KBANK_data.csv successfully.\n",
            "Processing file: ITI_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed ITI_data.csv successfully.\n",
            "Processing file: ASTERDM_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed ASTERDM_data.csv successfully.\n",
            "Processing file: RTNINDIA_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed RTNINDIA_data.csv successfully.\n",
            "Processing file: UJJIVANSFB_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed UJJIVANSFB_data.csv successfully.\n",
            "Processing file: POLYMED_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed POLYMED_data.csv successfully.\n",
            "Processing file: MCX_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed MCX_data.csv successfully.\n",
            "Processing file: KAJARIACER_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed KAJARIACER_data.csv successfully.\n",
            "Processing file: AMBUJACEM_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed AMBUJACEM_data.csv successfully.\n",
            "Processing file: GPIL_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed GPIL_data.csv successfully.\n",
            "Processing file: SKFINDIA_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed SKFINDIA_data.csv successfully.\n",
            "Processing file: SAPPHIRE_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed SAPPHIRE_data.csv successfully.\n",
            "Processing file: NCC_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed NCC_data.csv successfully.\n",
            "Processing file: DATAPATTNS_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed DATAPATTNS_data.csv successfully.\n",
            "Processing file: JKTYRE_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed JKTYRE_data.csv successfully.\n",
            "Processing file: NATIONALUM_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed NATIONALUM_data.csv successfully.\n",
            "Processing file: ATUL_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed ATUL_data.csv successfully.\n",
            "Processing file: FINPIPE_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed FINPIPE_data.csv successfully.\n",
            "Processing file: BIOCON_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed BIOCON_data.csv successfully.\n",
            "Processing file: GVT&D_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed GVT&D_data.csv successfully.\n",
            "Processing file: RAMCOCEM_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed RAMCOCEM_data.csv successfully.\n",
            "Processing file: UCOBANK_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed UCOBANK_data.csv successfully.\n",
            "Processing file: PIDILITIND_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed PIDILITIND_data.csv successfully.\n",
            "Processing file: OIL_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed OIL_data.csv successfully.\n",
            "Processing file: ENGINERSIN_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed ENGINERSIN_data.csv successfully.\n",
            "Processing file: RATNAMANI_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed RATNAMANI_data.csv successfully.\n",
            "Processing file: DLF_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed DLF_data.csv successfully.\n",
            "Processing file: MARUTI_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed MARUTI_data.csv successfully.\n",
            "Processing file: SBICARD_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed SBICARD_data.csv successfully.\n",
            "Processing file: AVANTIFEED_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed AVANTIFEED_data.csv successfully.\n",
            "Processing file: LINDEINDIA_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed LINDEINDIA_data.csv successfully.\n",
            "Processing file: ALKYLAMINE_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed ALKYLAMINE_data.csv successfully.\n",
            "Processing file: BBTC_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed BBTC_data.csv successfully.\n",
            "Processing file: GSFC_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed GSFC_data.csv successfully.\n",
            "Processing file: POWERINDIA_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed POWERINDIA_data.csv successfully.\n",
            "Processing file: IEX_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed IEX_data.csv successfully.\n",
            "Processing file: CHAMBLFERT_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed CHAMBLFERT_data.csv successfully.\n",
            "Processing file: PFIZER_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed PFIZER_data.csv successfully.\n",
            "Processing file: INDIAMART_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed INDIAMART_data.csv successfully.\n",
            "Processing file: HEROMOTOCO_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed HEROMOTOCO_data.csv successfully.\n",
            "Processing file: BANKBARODA_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed BANKBARODA_data.csv successfully.\n",
            "Processing file: GLAXO_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed GLAXO_data.csv successfully.\n",
            "Processing file: METROPOLIS_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed METROPOLIS_data.csv successfully.\n",
            "Processing file: CRAFTSMAN_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed CRAFTSMAN_data.csv successfully.\n",
            "Processing file: KARURVYSYA_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed KARURVYSYA_data.csv successfully.\n",
            "Processing file: IOB_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed IOB_data.csv successfully.\n",
            "Processing file: CGCL_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed CGCL_data.csv successfully.\n",
            "Processing file: INDUSTOWER_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed INDUSTOWER_data.csv successfully.\n",
            "Processing file: JUSTDIAL_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed JUSTDIAL_data.csv successfully.\n",
            "Processing file: ZYDUSLIFE_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed ZYDUSLIFE_data.csv successfully.\n",
            "Processing file: LICHSGFIN_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed LICHSGFIN_data.csv successfully.\n",
            "Processing file: JBCHEPHARM_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed JBCHEPHARM_data.csv successfully.\n",
            "Processing file: HDFCBANK_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed HDFCBANK_data.csv successfully.\n",
            "Processing file: CREDITACC_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed CREDITACC_data.csv successfully.\n",
            "Processing file: ENDURANCE_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed ENDURANCE_data.csv successfully.\n",
            "Processing file: FLUOROCHEM_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed FLUOROCHEM_data.csv successfully.\n",
            "Processing file: JSL_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed JSL_data.csv successfully.\n",
            "Processing file: CLEAN_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed CLEAN_data.csv successfully.\n",
            "Processing file: KOTAKBANK_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed KOTAKBANK_data.csv successfully.\n",
            "Processing file: SAMMAANCAP_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed SAMMAANCAP_data.csv successfully.\n",
            "Processing file: ABFRL_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed ABFRL_data.csv successfully.\n",
            "Processing file: CHALET_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed CHALET_data.csv successfully.\n",
            "Processing file: GRINDWELL_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed GRINDWELL_data.csv successfully.\n",
            "Processing file: MMTC_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed MMTC_data.csv successfully.\n",
            "Processing file: JSWSTEEL_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed JSWSTEEL_data.csv successfully.\n",
            "Processing file: POWERGRID_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed POWERGRID_data.csv successfully.\n",
            "Processing file: TORNTPOWER_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed TORNTPOWER_data.csv successfully.\n",
            "Processing file: CAMS_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed CAMS_data.csv successfully.\n",
            "Processing file: IRB_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed IRB_data.csv successfully.\n",
            "Processing file: COFORGE_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed COFORGE_data.csv successfully.\n",
            "Processing file: IRFC_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed IRFC_data.csv successfully.\n",
            "Processing file: HINDALCO_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed HINDALCO_data.csv successfully.\n",
            "Processing file: ADANIPORTS_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed ADANIPORTS_data.csv successfully.\n",
            "Processing file: BSOFT_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed BSOFT_data.csv successfully.\n",
            "Processing file: FORTIS_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed FORTIS_data.csv successfully.\n",
            "Processing file: M&MFIN_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed M&MFIN_data.csv successfully.\n",
            "Processing file: WELSPUNLIV_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed WELSPUNLIV_data.csv successfully.\n",
            "Processing file: LTF_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed LTF_data.csv successfully.\n",
            "Processing file: PNCINFRA_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed PNCINFRA_data.csv successfully.\n",
            "Processing file: ACE_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed ACE_data.csv successfully.\n",
            "Processing file: IDEA_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed IDEA_data.csv successfully.\n",
            "Processing file: HSCL_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed HSCL_data.csv successfully.\n",
            "Processing file: IFCI_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed IFCI_data.csv successfully.\n",
            "Processing file: ANANTRAJ_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed ANANTRAJ_data.csv successfully.\n",
            "Processing file: KALYANKJIL_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed KALYANKJIL_data.csv successfully.\n",
            "Processing file: SHYAMMETL_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed SHYAMMETL_data.csv successfully.\n",
            "Processing file: PATANJALI_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed PATANJALI_data.csv successfully.\n",
            "Processing file: MOTHERSON_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed MOTHERSON_data.csv successfully.\n",
            "Processing file: GMRAIRPORT_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed GMRAIRPORT_data.csv successfully.\n",
            "Processing file: SCHNEIDER_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed SCHNEIDER_data.csv successfully.\n",
            "Processing file: EASEMYTRIP_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed EASEMYTRIP_data.csv successfully.\n",
            "Processing file: SBILIFE_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed SBILIFE_data.csv successfully.\n",
            "Processing file: WHIRLPOOL_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed WHIRLPOOL_data.csv successfully.\n",
            "Processing file: NMDC_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed NMDC_data.csv successfully.\n",
            "Processing file: ANANDRATHI_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed ANANDRATHI_data.csv successfully.\n",
            "Processing file: LEMONTREE_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed LEMONTREE_data.csv successfully.\n",
            "Processing file: IRCON_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed IRCON_data.csv successfully.\n",
            "Processing file: CIPLA_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed CIPLA_data.csv successfully.\n",
            "Processing file: ICICIPRULI_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed ICICIPRULI_data.csv successfully.\n",
            "Processing file: MFSL_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed MFSL_data.csv successfully.\n",
            "Processing file: JSWENERGY_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed JSWENERGY_data.csv successfully.\n",
            "Processing file: BASF_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed BASF_data.csv successfully.\n",
            "Processing file: CONCOR_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed CONCOR_data.csv successfully.\n",
            "Processing file: INDHOTEL_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed INDHOTEL_data.csv successfully.\n",
            "Processing file: IIFL_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed IIFL_data.csv successfully.\n",
            "Processing file: ATGL_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed ATGL_data.csv successfully.\n",
            "Processing file: POONAWALLA_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed POONAWALLA_data.csv successfully.\n",
            "Processing file: LATENTVIEW_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed LATENTVIEW_data.csv successfully.\n",
            "Processing file: JKCEMENT_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed JKCEMENT_data.csv successfully.\n",
            "Processing file: BAJFINANCE_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed BAJFINANCE_data.csv successfully.\n",
            "Processing file: GODREJAGRO_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed GODREJAGRO_data.csv successfully.\n",
            "Processing file: ASTRAL_data.csv\n",
            "Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'Normalised Close', 'Return Ratio', 'Percentage Change Open', 'Percentage Change High', 'Percentage Change Low', 'Sector', 'Sector Encoded', 'Stock_Movement_Label'] (20 columns)\n",
            "✅ Processed ASTRAL_data.csv successfully.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Define directories\n",
        "file_path = \"/content/Data_is_here\"\n",
        "save_path = \"/content/Preprocessed_data\"\n",
        "\n",
        "# Ensure save directory exists\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "def set_data(file):\n",
        "    file_full_path = os.path.join(file_path, file)\n",
        "\n",
        "    # Check if file is empty\n",
        "    if os.stat(file_full_path).st_size == 0:\n",
        "        print(f\"⚠️ Skipping empty file: {file}\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        # Read CSV\n",
        "        df = pd.read_csv(file_full_path)\n",
        "\n",
        "        # Fill NaN values with 0\n",
        "        df.fillna(0, inplace=True)\n",
        "\n",
        "        # Drop the first two rows (assuming metadata)\n",
        "        df = df.iloc[2:].reset_index(drop=True)\n",
        "\n",
        "        # Remove unnamed columns\n",
        "        df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
        "\n",
        "        # Debugging: Print columns before renaming\n",
        "        print(f\"Processing file: {file}\")\n",
        "        print(f\"Original columns: {df.columns.tolist()} ({len(df.columns)} columns)\")\n",
        "\n",
        "        # Expected column names\n",
        "        expected_columns = [\n",
        "            \"Date\",\n",
        "            \"Close\", \"High\", \"Low\", \"Open\",\n",
        "            \"Volume\",\n",
        "            \"MA_5\", \"MA_15\", \"MA_10\", \"MA_20\", \"MA_25\", \"MA_30\",\n",
        "            \"Normalized_Close\",\n",
        "            \"Return_Ratio\", \"Percentage_Change_Open\", \"Percentage_Change_High\", \"Percentage_Change_Low\",\n",
        "            \"Sector\",\n",
        "            \"Sector_Encoded\",\n",
        "            \"Stock_Movement_Label\"\n",
        "        ]\n",
        "\n",
        "        # Fix: Only rename columns if the count matches\n",
        "        if len(df.columns) == len(expected_columns):\n",
        "            df.columns = expected_columns\n",
        "        else:\n",
        "            print(f\"❌ Column mismatch in {file}. Expected {len(expected_columns)} but found {len(df.columns)}. Skipping renaming.\")\n",
        "            return  # Skip further processing\n",
        "\n",
        "        # Drop unnecessary columns\n",
        "        df = df.drop(columns=[\"Date\", \"Sector\", \"Volume\", \"Normalized_Close\"], errors='ignore')\n",
        "\n",
        "        # Convert to numeric\n",
        "        for col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "        # Drop rows with NaN values and reset index\n",
        "        df = df.dropna().reset_index(drop=True)\n",
        "\n",
        "        # Save preprocessed file\n",
        "        df.to_csv(os.path.join(save_path, file), index=False)\n",
        "        print(f\"✅ Processed {file} successfully.\")\n",
        "\n",
        "    except pd.errors.EmptyDataError:\n",
        "        print(f\"⚠️ Skipping {file}: File is empty or unreadable.\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error processing {file}: {e}\")\n",
        "\n",
        "# Process all files in the directory\n",
        "files = os.listdir(file_path)\n",
        "\n",
        "if not files:\n",
        "    print(\"🚨 No CSV files found in the directory!\")\n",
        "else:\n",
        "    for file in files:\n",
        "        if file.endswith(\".csv\"):  # Only process CSV files\n",
        "            set_data(file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ViQR0ZV_M4gq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cea39e9d-bca5-4370-afac-4138f6be2066"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of companies (CSV files): 444\n",
            "        Close        High         Low        Open        MA_5  MA_15  MA_10  \\\n",
            "0  128.637039  130.686404  128.290219  130.686404    0.000000    0.0    0.0   \n",
            "1  128.542450  129.772076  128.321752  128.983859    0.000000    0.0    0.0   \n",
            "2  129.236084  130.339588  128.857742  129.898192    0.000000    0.0    0.0   \n",
            "3  131.159348  133.366356  130.245021  130.907115    0.000000    0.0    0.0   \n",
            "4  131.001709  132.199802  130.245026  131.127819  129.715326    0.0    0.0   \n",
            "\n",
            "   MA_20  MA_25  MA_30  Return_Ratio  Percentage_Change_Open  \\\n",
            "0    0.0    0.0    0.0      0.000000                0.015931   \n",
            "1    0.0    0.0    0.0     -0.000735                0.003434   \n",
            "2    0.0    0.0    0.0      0.005396                0.005123   \n",
            "3    0.0    0.0    0.0      0.014882               -0.001923   \n",
            "4    0.0    0.0    0.0     -0.001202                0.000963   \n",
            "\n",
            "   Percentage_Change_High  Percentage_Change_Low  Sector_Encoded  \\\n",
            "0                0.015931              -0.002696            14.0   \n",
            "1                0.009566              -0.001717            14.0   \n",
            "2                0.008539              -0.002928            14.0   \n",
            "3                0.016827              -0.006971            14.0   \n",
            "4                0.009146              -0.005776            14.0   \n",
            "\n",
            "   Stock_Movement_Label  \n",
            "0                     0  \n",
            "1                     0  \n",
            "2                     0  \n",
            "3                     1  \n",
            "4                     1  \n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "file_path = \"/content/Data_is_here\"\n",
        "\n",
        "if os.path.exists(file_path):\n",
        "    files = [file for file in os.listdir(file_path) if file.endswith(\".csv\")]\n",
        "    print(f\"Number of companies (CSV files): {len(files)}\")\n",
        "else:\n",
        "    print(f\"Directory {file_path} does not exist.\")\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"/content/Preprocessed_data/POWERGRID_data.csv\")  # Replace with an actual filename\n",
        "print(df.head())  # Check if \"Stock_Movement_Label\" is updated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "71kUsT_2LPOb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "417e98a9-15b8-495d-ef14-640e83900744"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        Price               Close                High                 Low  \\\n",
            "0      Ticker        POWERGRID.NS        POWERGRID.NS        POWERGRID.NS   \n",
            "1        Date                 NaN                 NaN                 NaN   \n",
            "2  2022-01-10   128.6370391845703  130.68640377942253  128.29021850612708   \n",
            "3  2022-01-11  128.54244995117188  129.77207639968722  128.32175171550662   \n",
            "4  2022-01-12    129.236083984375  130.33958803879312  128.85774230295706   \n",
            "\n",
            "                 Open        Volume  MA_5  MA_10  MA_15  MA_20  MA_25  MA_30  \\\n",
            "0        POWERGRID.NS  POWERGRID.NS   NaN    NaN    NaN    NaN    NaN    NaN   \n",
            "1                 NaN           NaN   NaN    NaN    NaN    NaN    NaN    NaN   \n",
            "2  130.68640377942253      11085710   NaN    NaN    NaN    NaN    NaN    NaN   \n",
            "3  128.98385925155756       7208789   NaN    NaN    NaN    NaN    NaN    NaN   \n",
            "4  129.89819154864816      11018139   NaN    NaN    NaN    NaN    NaN    NaN   \n",
            "\n",
            "   Normalised Close  Return Ratio  Percentage Change Open  \\\n",
            "0               NaN           NaN                     NaN   \n",
            "1               NaN           NaN                     NaN   \n",
            "2         -1.065506           NaN                0.015931   \n",
            "3         -1.066808     -0.000735                0.003434   \n",
            "4         -1.057257      0.005396                0.005123   \n",
            "\n",
            "   Percentage Change High  Percentage Change Low Sector  Sector Encoded  \\\n",
            "0                     NaN                    NaN    NaN             NaN   \n",
            "1                     NaN                    NaN    NaN             NaN   \n",
            "2                0.015931              -0.002696  Power            14.0   \n",
            "3                0.009566              -0.001717  Power            14.0   \n",
            "4                0.008539              -0.002928  Power            14.0   \n",
            "\n",
            "   Stock_Movement_Label  \n",
            "0                     1  \n",
            "1                     0  \n",
            "2                     0  \n",
            "3                     0  \n",
            "4                     0  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/Data_is_here/POWERGRID_data.csv\")  # Replace with an actual filename\n",
        "print(df.head())  # Check if \"Stock_Movement_Label\" is updated\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30QSo7yWM_iv"
      },
      "outputs": [],
      "source": [
        "#graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-uy7TklENU0E"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# import torch\n",
        "\n",
        "# # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# # class StockDataLoader:\n",
        "#     def __init__(self, data_dir, window_size):\n",
        "#         self.data_dir = data_dir\n",
        "#         self.window_size = window_size\n",
        "#         self.file_paths = [\n",
        "#             os.path.join(data_dir, f)\n",
        "#             for f in os.listdir(data_dir)\n",
        "#             if f.endswith('.csv')\n",
        "#         ]\n",
        "#         self.data, self.stock_names, self.labels, self.returns = self._load_all_data()\n",
        "#         self.num_stocks = len(self.data)\n",
        "\n",
        "#     def _load_all_data(self):\n",
        "#         \"\"\"Load all stock data into memory, separating features, labels, and next-day returns.\"\"\"\n",
        "#         data = []\n",
        "#         stock_names = []\n",
        "#         labels = []  # For stock movement (y_move)\n",
        "#         returns = []  # For next day's return ratio (y_return)\n",
        "\n",
        "#         for path in self.file_paths:\n",
        "#             df = pd.read_csv(path)\n",
        "#             df = df.loc[:, ~df.columns.str.contains('^Unnamed')]  # Remove unnamed columns\n",
        "\n",
        "#             # Ensure the dataset contains a 'Close' column\n",
        "#             if 'Close' not in df.columns:\n",
        "#                 raise ValueError(f\"Dataset {path} does not contain a 'Close' column.\")\n",
        "\n",
        "#             # Separate features and labels (assuming labels are in the last column)\n",
        "#             feature_columns = df.columns[:-1]  # All columns except the last one (features)\n",
        "#             label_column = df.columns[-1]  # Last column is the movement label\n",
        "\n",
        "#             stock_data = df[feature_columns].values  # Features (timesteps, features)\n",
        "#             label_data = df[label_column].values   # Movement labels (timesteps, 1)\n",
        "\n",
        "#             # Compute next day's return ratio\n",
        "#             close_prices = df['Close'].values\n",
        "#             return_ratio = np.zeros_like(close_prices)  # Initialize array for returns\n",
        "#             return_ratio[:-1] = (close_prices[1:] / close_prices[:-1]) - 1  # (P_t+1 / P_t) - 1\n",
        "#             return_ratio[-1] = 0  # No next day return for the last entry\n",
        "\n",
        "#             stock_names.append(os.path.basename(path))  # Store the stock name (filename)\n",
        "#             data.append(stock_data)\n",
        "#             labels.append(label_data)  # Store movement labels\n",
        "#             returns.append(return_ratio)  # Store return ratios\n",
        "\n",
        "#         return data, stock_names, labels, returns\n",
        "\n",
        "#     def __iter__(self):\n",
        "#         \"\"\"Generator that yields batches using a sliding window with batch size = number of stocks.\"\"\"\n",
        "#         while True:\n",
        "#             batch = []\n",
        "#             stock_batch_names = []\n",
        "#             batch_labels = []  # Movement labels (single value for the next day)\n",
        "#             batch_returns = []  # Next day's return ratio (single value)\n",
        "\n",
        "#             for i in range(self.num_stocks):\n",
        "#                 stock_data = self.data[i]\n",
        "#                 stock_labels = self.labels[i]\n",
        "#                 stock_returns = self.returns[i]  # Get return ratios\n",
        "\n",
        "#                 # Randomly choose a start index for the sliding window, ensuring we don't go out of bounds\n",
        "#                 start_idx = np.random.randint(0, len(stock_data) - self.window_size)\n",
        "\n",
        "#                 window = stock_data[start_idx:start_idx + self.window_size]  # Sliding window of features\n",
        "\n",
        "#                 # The next day's values for movement and return\n",
        "#                 next_day_label = stock_labels[start_idx + self.window_size]  # Movement label (next day)\n",
        "#                 next_day_return = stock_returns[start_idx + self.window_size]  # Return ratio (next day)\n",
        "\n",
        "#                 # Append the single value of movement and return for the next day\n",
        "#                 batch.append(window)\n",
        "#                 batch_labels.append(next_day_label)  # Append the next day's movement label\n",
        "#                 batch_returns.append(next_day_return)  # Append the next day's return ratio\n",
        "#                 stock_batch_names.append(self.stock_names[i][:-9])  # Get the stock name\n",
        "\n",
        "#             # Stack to form (num_stocks, window_size, features) and labels (num_stocks,)\n",
        "#             batch_tensor = torch.tensor(np.stack(batch), dtype=torch.float32).to(device)\n",
        "#             labels_tensor = torch.tensor(np.stack(batch_labels), dtype=torch.float32).to(device)  # Movement labels (num_stocks,)\n",
        "#             returns_tensor = torch.tensor(np.stack(batch_returns), dtype=torch.float32).to(device)  # Return ratios (num_stocks,)\n",
        "\n",
        "#             # Yield features, stock names, movement labels, and return ratios\n",
        "#             yield batch_tensor, stock_batch_names, labels_tensor, returns_tensor\n",
        "\n",
        "# # Example usage\n",
        "# data_dir = '/content/Preprocessed_data'  # Directory containing 403 CSV files\n",
        "# window_size = 30  # Number of timesteps per bdonatch for each stock\n",
        "\n",
        "# # Initialize the data loader\n",
        "# data_loader = StockDataLoader(data_dir, window_size)\n",
        "\n",
        "# # Create an iterator\n",
        "# data_iter = iter(data_loader)\n",
        "\n",
        "# # Fetch one batch\n",
        "# batch, stock_names, y_move, y_return = next(data_iter)\n",
        "\n",
        "# print(batch.device)       # Output: torch.Size([num_stocks, 30, num_features])\n",
        "# # print(stock_names)  # Number of stock names\n",
        "# print(y_move.device)      # Output: torch.Size([num_stocks,]) (next day's movement)\n",
        "# print(y_return.device)    # Output: torch.Size([num_stocks,]) (next day's return)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEKJYWWUN1n2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "p8SA5DYCS22K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12c73a17-2137-4543-a87a-977eaeb99610"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n",
            "cuda:0\n",
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class StockDataLoader:\n",
        "    def __init__(self, data_dir, window_size):\n",
        "        self.data_dir = data_dir\n",
        "        self.window_size = window_size\n",
        "        self.file_paths = [\n",
        "            os.path.join(data_dir, f)\n",
        "            for f in os.listdir(data_dir)\n",
        "            if f.endswith('.csv')\n",
        "        ]\n",
        "        self.data, self.stock_names, self.labels, self.returns = self._load_all_data()\n",
        "        self.num_stocks = len(self.data)\n",
        "\n",
        "    def _load_all_data(self):\n",
        "        \"\"\"Load all stock data into memory, separating features, labels, and next-day returns.\"\"\"\n",
        "        data = []\n",
        "        stock_names = []\n",
        "        labels = []  # For stock movement (y_move)\n",
        "        returns = []  # For next day's return ratio (y_return)\n",
        "\n",
        "        for path in self.file_paths:\n",
        "            df = pd.read_csv(path)\n",
        "            df = df.loc[:, ~df.columns.str.contains('^Unnamed')]  # Remove unnamed columns\n",
        "\n",
        "            # Ensure the dataset contains a 'Close' column\n",
        "            if 'Close' not in df.columns:\n",
        "                raise ValueError(f\"Dataset {path} does not contain a 'Close' column.\")\n",
        "\n",
        "            # Separate features and labels (assuming labels are in the last column)\n",
        "            feature_columns = df.columns[:-1]  # All columns except the last one (features)\n",
        "            label_column = df.columns[-1]  # Last column is the movement label\n",
        "\n",
        "            stock_data = df[feature_columns].values  # Features (timesteps, features)\n",
        "            label_data = df[label_column].values   # Movement labels (timesteps, 1)\n",
        "\n",
        "            # Compute next day's return ratio\n",
        "            close_prices = df['Close'].values\n",
        "            return_ratio = np.zeros_like(close_prices)  # Initialize array for returns\n",
        "            return_ratio[:-1] = (close_prices[1:] / close_prices[:-1]) - 1  # (P_t+1 / P_t) - 1\n",
        "            # return_ratio[-1] = 0  # No next day return for the last entry\n",
        "\n",
        "            stock_names.append(os.path.basename(path))  # Store the stock name (filename)\n",
        "            data.append(stock_data)\n",
        "            labels.append(label_data)  # Store movement labels\n",
        "            returns.append(return_ratio)  # Store return ratios\n",
        "\n",
        "        return data, stock_names, labels, returns\n",
        "\n",
        "    def __iter__(self):\n",
        "        \"\"\"Generator that yields batches using a sliding window with batch size = number of stocks.\"\"\"\n",
        "        while True:\n",
        "            batch = []\n",
        "            stock_batch_names = []\n",
        "            batch_labels = []  # Movement labels (single value for the next day)\n",
        "            batch_returns = []  # Next day's return ratio (single value)\n",
        "\n",
        "            for i in range(self.num_stocks):\n",
        "                stock_data = self.data[i]\n",
        "                stock_labels = self.labels[i]\n",
        "                stock_returns = self.returns[i]  # Get return ratios\n",
        "\n",
        "                # Randomly choose a start index for the sliding window, ensuring we don't go out of bounds\n",
        "                start_idx = np.random.randint(0, len(stock_data) - self.window_size)\n",
        "\n",
        "                window = stock_data[start_idx:start_idx + self.window_size]  # Sliding window of features\n",
        "\n",
        "                # The next day's values for movement and return\n",
        "                next_day_label = stock_labels[start_idx + self.window_size]  # Movement label (next day)\n",
        "                next_day_return = stock_returns[start_idx + self.window_size]  # Return ratio (next day)\n",
        "\n",
        "                # Append the single value of movement and return for the next day\n",
        "                batch.append(window)\n",
        "                batch_labels.append(next_day_label)  # Append the next day's movement label\n",
        "                batch_returns.append(next_day_return)  # Append the next day's return ratio\n",
        "                stock_batch_names.append(self.stock_names[i][:-9])  # Get the stock name\n",
        "\n",
        "            # Stack to form (num_stocks, window_size, features) and labels (num_stocks,)\n",
        "            batch_tensor = torch.tensor(np.stack(batch), dtype=torch.float32).to(device)\n",
        "            labels_tensor = torch.tensor(np.stack(batch_labels), dtype=torch.float32).to(device)  # Movement labels (num_stocks,)\n",
        "            returns_tensor = torch.tensor(np.stack(batch_returns), dtype=torch.float32).to(device)  # Return ratios (num_stocks,)\n",
        "\n",
        "            # Yield features, stock names, movement labels, and return ratios\n",
        "            yield batch_tensor, stock_batch_names, labels_tensor, returns_tensor\n",
        "\n",
        "# Example usage\n",
        "data_dir = 'Preprocessed_data'  # Directory containing 403 CSV files\n",
        "window_size = 30  # Number of timesteps per batch for each stock\n",
        "\n",
        "# Initialize the data loader\n",
        "data_loader = StockDataLoader(data_dir, window_size)\n",
        "\n",
        "# Create an iterator\n",
        "data_iter = iter(data_loader)\n",
        "\n",
        "# Fetch one batch\n",
        "batch, stock_names, y_move, y_return = next(data_iter)\n",
        "\n",
        "print(batch.device)       # Output: torch.Size([num_stocks, 30, num_features])\n",
        "# print(stock_names)  # Number of stock names\n",
        "print(y_move.device)      # Output: torch.Size([num_stocks,]) (next day's movement)\n",
        "print(y_return.device)    # Output: torch.Size([num_stocks,]) (next day's return)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "llDA1UTBN90t"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch_geometric.nn import GATConv\n",
        "\n",
        "class StockGAT(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim, out_dim, heads=4, dropout=0.1):\n",
        "        \"\"\"\n",
        "        Graph Attention Network (GAT) for modeling stock relationships within a sector.\n",
        "\n",
        "        - in_dim: Input embedding size (e.g., 64)\n",
        "        - hidden_dim: Size of hidden layer\n",
        "        - out_dim: Output embedding size (final node representation)\n",
        "        - heads: Number of attention heads\n",
        "        - dropout: Dropout rate\n",
        "        \"\"\"\n",
        "        super(StockGAT, self).__init__()\n",
        "        self.gat1 = GATConv(in_dim, hidden_dim, heads=heads, dropout=dropout)\n",
        "        self.gat2 = GATConv(hidden_dim * heads, out_dim, heads=1, dropout=dropout)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        \"\"\"\n",
        "        x: Tensor of stock embeddings (num_stocks, num_weeks, hidden_dim)\n",
        "        edge_index: Graph connections (2, num_edges)\n",
        "        \"\"\"\n",
        "        # Flatten the input tensor for GAT (num_stocks * num_weeks, hidden_dim)\n",
        "        x = x.view(-1, x.size(-1)).to(device)  # Ensure x is on device\n",
        "        edge_index = edge_index.to(device)  # Explicitly move edge_index\n",
        "        # edge_index = edge_index.to(device)\n",
        "        # Apply GAT layers\n",
        "        x = self.gat1(x, edge_index)\n",
        "        x = self.relu(x)\n",
        "        x = self.gat2(x, edge_index)\n",
        "\n",
        "        # Reshape the output back to (num_stocks, num_weeks, hidden_dim)\n",
        "        x = x.view(-1, 6, x.size(-1))  # Assuming 6 weeks per stock\n",
        "\n",
        "        return x\n",
        "\n",
        "def process_batch(batch_stock_names, ai_batch, model, intra_sector_stock_2_stock):\n",
        "    \"\"\"\n",
        "    Process a batch of stock names and their respective neighbors through the StockGAT model.\n",
        "\n",
        "    - batch_stock_names: List of stock names in the current batch.\n",
        "    - ai_batch: Tensor containing the embeddings of the stocks, shape (batch_size, num_weeks, hidden_dim).\n",
        "    - model: The GAT model.\n",
        "    - intra_sector_stock_2_stock: Dictionary mapping a stock to its neighbors.\n",
        "\n",
        "    Returns:\n",
        "    - A tensor of shape (batch_size, num_weeks, hidden_dim) representing the output embeddings.\n",
        "    \"\"\"\n",
        "    # Create a list to store the edge connections for the entire batch\n",
        "    edge_index_list = []\n",
        "\n",
        "    for idx, name in enumerate(batch_stock_names):\n",
        "        # Find neighbors of the current stock\n",
        "        neighbour_names = intra_sector_stock_2_stock.get(name, [])\n",
        "\n",
        "        # Add edges for the stock and its neighbors\n",
        "        for neighbour in neighbour_names:\n",
        "            neighbour_idx = batch_stock_names.index(neighbour)\n",
        "            edge_index_list.append([idx, neighbour_idx])  # Add edge from the stock to its neighbor\n",
        "            edge_index_list.append([neighbour_idx, idx])  # Add edge from the neighbor to the stock\n",
        "\n",
        "    # Convert edge_index list to a tensor\n",
        "    edge_index = torch.tensor(edge_index_list, dtype=torch.long).t().contiguous()\n",
        "    edge_index = edge_index.to(device)\n",
        "    # Pass the embeddings through the model\n",
        "    gi_batch = model(ai_batch, edge_index)\n",
        "\n",
        "    return gi_batch\n",
        "\n",
        "# # Example usage\n",
        "# batch_stock_names = [\"AAPL\", \"GOOG\", \"AMZN\", \"MSFT\"]\n",
        "# ai_batch = torch.randn(4, 6, 64)  # Simulated embeddings for 4 stocks over 6 weeks\n",
        "# intra_sector_stock_2_stock = {\n",
        "#     \"AAPL\": [\"GOOG\", \"MSFT\"],\n",
        "#     \"GOOG\": [\"AAPL\", \"AMZN\"],\n",
        "#     \"AMZN\": [\"GOOG\", \"MSFT\"],\n",
        "#     \"MSFT\": [\"AAPL\", \"AMZN\"]\n",
        "# }\n",
        "# model = StockGAT(in_dim=64, hidden_dim=64, out_dim=64)  # Example GAT model\n",
        "\n",
        "# gi_batch = process_batch(batch_stock_names, ai_batch, model, intra_sector_stock_2_stock)\n",
        "# # print(output_embeddings.shape)  # Expected shape: (4, 6, 64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ptpeL1BqPY3T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5677026d-33d0-44f3-dee4-fb41d4a78475"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SCyf0MhpOGnL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class AttentiveLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=64, days_per_week=5):\n",
        "        super(AttentiveLSTM, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.days_per_week = days_per_week\n",
        "        # Replacing GRU with LSTM\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
        "        self.attention = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: Tensor of shape (batch_size, total_days, input_dim)\n",
        "        \"\"\"\n",
        "        batch_size, total_days, _ = x.shape\n",
        "\n",
        "        # Number of weeks\n",
        "        num_weeks = total_days // self.days_per_week\n",
        "\n",
        "        # Reshape to (batch_size, num_weeks, days_per_week, input_dim)\n",
        "        x_reshaped = x.view(batch_size, num_weeks, self.days_per_week, -1)\n",
        "\n",
        "        weekly_embeddings = []\n",
        "\n",
        "        for i in range(num_weeks):\n",
        "            week_input = x_reshaped[:, i, :, :]  # Shape: (batch_size, days_per_week, input_dim)\n",
        "\n",
        "            # LSTM forward pass\n",
        "            h_seq, (h_n, c_n) = self.lstm(week_input)  # (batch_size, days_per_week, hidden_dim)\n",
        "\n",
        "            # Compute attention scores\n",
        "            attn_weights = torch.tanh(self.attention(h_seq))  # (batch_size, days_per_week, 1)\n",
        "            attn_weights = torch.softmax(attn_weights, dim=1)  # Normalize over time dimension\n",
        "\n",
        "            # Compute weighted sum for weekly embeddings\n",
        "            weekly_embedding = torch.sum(attn_weights * h_seq, dim=1)  # (batch_size, hidden_dim)\n",
        "\n",
        "            weekly_embeddings.append(weekly_embedding)\n",
        "\n",
        "        # Stack to get final shape (batch_size, num_weeks, hidden_dim)\n",
        "        weekly_embeddings = torch.stack(weekly_embeddings, dim=1)\n",
        "\n",
        "        return weekly_embeddings\n",
        "\n",
        "# Example usage\n",
        "# batch_size = 403\n",
        "# total_days = 30  # 30 days\n",
        "# input_dim = 15  # Feature dimension per day\n",
        "\n",
        "# model = AttentiveLSTM(input_dim=input_dim, hidden_dim=64, days_per_week=5)\n",
        "# dummy_input = torch.randn(batch_size, total_days, input_dim)  # Simulated daily features\n",
        "# weekly_embeddings = model(dummy_input)  # Output shape: (batch_size, num_weeks, hidden_dim)\n",
        "\n",
        "# print(weekly_embeddings.shape)  # Expected output: (403, 6, 64)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "tGvo5Bo-Qznq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LongTermAttentionBlock(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, t=5):\n",
        "        \"\"\"\n",
        "        A block that processes `ai_batch` (stock-specific embeddings) and `gi_batch` (sector-level embeddings)\n",
        "        to compute the long-term trends using an attention mechanism.\n",
        "\n",
        "        - input_dim: The dimension of the input embeddings (e.g., 64).\n",
        "        - hidden_dim: The size of the hidden layer.\n",
        "        - t: The look-back period (i.e., number of past weeks to consider for attention).\n",
        "        \"\"\"\n",
        "        super(LongTermAttentionBlock, self).__init__()\n",
        "        self.t = t\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Learnable parameters W and V for the attention mechanism\n",
        "        self.W = nn.Parameter(torch.randn(input_dim, hidden_dim))\n",
        "        self.V = nn.Parameter(torch.randn(hidden_dim, 1))\n",
        "\n",
        "    def forward(self, ai_batch, gi_batch):\n",
        "        \"\"\"\n",
        "        ai_batch: Tensor of shape (batch_size, t, input_dim) representing stock-specific embeddings.\n",
        "        gi_batch: Tensor of shape (batch_size, t, input_dim) representing sector-level embeddings.\n",
        "\n",
        "        Returns:\n",
        "        - long_term_ai: Long-term embedding for stock-level trends (shape: (batch_size, hidden_dim)).\n",
        "        - long_term_gi: Long-term embedding for sector-level trends (shape: (batch_size, hidden_dim)).\n",
        "        \"\"\"\n",
        "        # Process the ai_batch (stock-specific embeddings)\n",
        "        ai_attention_weights = self.compute_attention(ai_batch)  # (batch_size, t)\n",
        "\n",
        "        # Expand the attention weights to shape (batch_size, t, 1) for broadcasting\n",
        "        ai_attention_weights = ai_attention_weights.unsqueeze(-1)  # (batch_size, t, 1)\n",
        "\n",
        "        # Compute weighted sum for ai_batch long-term embedding\n",
        "        long_term_ai = torch.sum(ai_attention_weights * ai_batch, dim=1)  # (batch_size, input_dim)\n",
        "\n",
        "        # Process the gi_batch (sector-level embeddings)\n",
        "        gi_attention_weights = self.compute_attention(gi_batch)  # (batch_size, t)\n",
        "\n",
        "        # Expand the attention weights to shape (batch_size, t, 1) for broadcasting\n",
        "        gi_attention_weights = gi_attention_weights.unsqueeze(-1)  # (batch_size, t, 1)\n",
        "\n",
        "        # Compute weighted sum for gi_batch long-term embedding\n",
        "        long_term_gi = torch.sum(gi_attention_weights * gi_batch, dim=1)  # (batch_size, input_dim)\n",
        "\n",
        "        return long_term_ai, long_term_gi\n",
        "\n",
        "    def compute_attention(self, u_batch):\n",
        "        \"\"\"\n",
        "        Compute attention weights using the formula:\n",
        "\n",
        "        αj = exp(W^T * tanh(V * uj)) / sum(exp(W^T * tanh(V * uk))) for k in (i-t, ..., i-1)\n",
        "\n",
        "        u_batch: Tensor of shape (batch_size, t, input_dim)\n",
        "        \"\"\"\n",
        "        batch_size, t, input_dim = u_batch.shape\n",
        "\n",
        "        # Apply the attention mechanism\n",
        "        u_flat = u_batch.view(batch_size * t, input_dim)  # Flatten to shape (batch_size * t, input_dim)\n",
        "        u_tanh = torch.tanh(torch.matmul(u_flat, self.W))  # (batch_size * t, hidden_dim)\n",
        "        attention_scores = torch.matmul(u_tanh, self.V)  # (batch_size * t, 1)\n",
        "\n",
        "        # Reshape attention_scores back to (batch_size, t)\n",
        "        attention_scores = attention_scores.view(batch_size, t)  # (batch_size, t)\n",
        "\n",
        "        # Apply softmax to get attention weights\n",
        "        attention_weights = torch.softmax(attention_scores, dim=1)  # (batch_size, t)\n",
        "\n",
        "        return attention_weights\n",
        "\n",
        "# # Example usage\n",
        "# batch_size = 407\n",
        "# t = 6  # Look-back period for long-term embedding\n",
        "# input_dim = 64\n",
        "# hidden_dim = 64\n",
        "\n",
        "# # Simulate the ai_batch (stock-specific) and gi_batch (sector-level) embeddings\n",
        "# ai_batch = torch.randn(batch_size, t, input_dim)\n",
        "# gi_batch = torch.randn(batch_size, t, input_dim)\n",
        "\n",
        "# # Initialize the long-term attention block\n",
        "# long_term_block = LongTermAttentionBlock(input_dim=input_dim, hidden_dim=hidden_dim, t=t)\n",
        "\n",
        "# # Get the long-term embeddings for both stock-level and sector-level trends\n",
        "# long_term_ai, long_term_gi = long_term_block(ai_batch, gi_batch)\n",
        "\n",
        "# print(\"Long-term stock-level trend:\", long_term_ai.shape)  # (batch_size, hidden_dim)\n",
        "# print(\"Long-term sector-level trend:\", long_term_gi.shape)  # (batch_size, hidden_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "T2VCV2fARu-3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SectorEmbeddingBlock(nn.Module):\n",
        "    def __init__(self, input_dim, num_sectors, t=5):\n",
        "        \"\"\"\n",
        "        A block that processes `long_term_gi_batch` (sector-level embeddings) and computes the sector-level embeddings\n",
        "        using max pooling.\n",
        "\n",
        "        - input_dim: The dimension of the input embeddings (e.g., 64).\n",
        "        - num_sectors: Number of sectors.\n",
        "        - t: The look-back period (i.e., number of past weeks to consider for attention).\n",
        "        \"\"\"\n",
        "        super(SectorEmbeddingBlock, self).__init__()\n",
        "        self.t = t\n",
        "        self.input_dim = input_dim\n",
        "        self.num_sectors = num_sectors\n",
        "\n",
        "    def forward(self, long_term_gi_batch, batch_names, sector_dict):\n",
        "        \"\"\"\n",
        "        long_term_gi_batch: Tensor of shape (batch_size, input_dim) representing long-term sector embeddings.\n",
        "        batch_names: List of stock names corresponding to each embedding in long_term_gi_batch.\n",
        "        sector_dict: Dictionary mapping sector names to a list of stock names in that sector.\n",
        "\n",
        "        Returns:\n",
        "        - sector_embeddings: Tensor of shape (num_sectors, input_dim) representing the pooled\n",
        "          embeddings for each sector.\n",
        "        \"\"\"\n",
        "        # Initialize a tensor to store sector embeddings\n",
        "        sector_embeddings = torch.zeros(self.num_sectors, self.input_dim).to(long_term_gi_batch.device)\n",
        "\n",
        "        # Step 1: Max pooling across stocks within each sector\n",
        "        for sector_idx, sector in enumerate(sector_dict.keys()):\n",
        "            # Get the list of stock names in the current sector\n",
        "            sector_stocks = sector_dict[sector]\n",
        "\n",
        "            # Get the indices of stocks that belong to the current sector\n",
        "            sector_indices = [i for i, name in enumerate(batch_names) if name in sector_stocks]\n",
        "\n",
        "            # Extract the embeddings for the stocks in this sector\n",
        "            sector_embeddings_tensor = long_term_gi_batch[sector_indices]  # (num_stocks_in_sector, input_dim)\n",
        "\n",
        "            # Max pooling across the time dimension (axis=0)\n",
        "            # Check if sector_embeddings_tensor is empty\n",
        "            if sector_embeddings_tensor.shape[0] > 0:\n",
        "                sector_embeddings[sector_idx] = torch.max(sector_embeddings_tensor, dim=0)[0]\n",
        "            else:\n",
        "                print(f\"Empty tensor encountered for sector {sector_idx}\")\n",
        "                # Handle empty tensor case (e.g., skip or use a default embedding)\n",
        "        return sector_embeddings\n",
        "\n",
        "# # Example usage\n",
        "# batch_size = 403\n",
        "# t = 6  # Look-back period for long-term embedding\n",
        "# input_dim = 64\n",
        "# num_sectors = 5  # Number of sectors\n",
        "\n",
        "# # Simulate the long_term_gi_batch (sector-level) embeddings\n",
        "# long_term_gi_batch = torch.randn(batch_size, input_dim)\n",
        "# # Simulate the stock names for each embedding\n",
        "# batch_names = [f\"stock_{i}\" for i in range(batch_size)]\n",
        "\n",
        "# # Simulate a sector dictionary with some dummy data\n",
        "# sector_dict = {\n",
        "#     \"sector_1\": [f\"stock_{i}\" for i in range(0, 100)],\n",
        "#     \"sector_2\": [f\"stock_{i}\" for i in range(100, 200)],\n",
        "#     \"sector_3\": [f\"stock_{i}\" for i in range(200, 300)],\n",
        "#     \"sector_4\": [f\"stock_{i}\" for i in range(300, 400)],\n",
        "#     \"sector_5\": [f\"stock_{i}\" for i in range(400, 403)]\n",
        "# }\n",
        "\n",
        "# # Initialize the sector embedding block\n",
        "# sector_embedding_block = SectorEmbeddingBlock(input_dim=input_dim, num_sectors=num_sectors, t=t)\n",
        "\n",
        "# # Get the sector-level embeddings\n",
        "# sector_embeddings = sector_embedding_block(long_term_gi_batch, batch_names, sector_dict)\n",
        "\n",
        "# print(\"Sector embeddings:\", sector_embeddings.shape)  # (num_sectors, input_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "eGIxOpV2cvX3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GATConv\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "class GATEmbeddingBlock(nn.Module):\n",
        "    def __init__(self, input_dim, num_sectors, num_heads=1):\n",
        "        \"\"\"\n",
        "        A block that takes sector embeddings and applies Graph Attention Network (GAT)\n",
        "\n",
        "        - input_dim: The dimension of the input embeddings (e.g., 64).\n",
        "        - num_sectors: Number of sectors.\n",
        "        - num_heads: The number of attention heads for GAT.\n",
        "        \"\"\"\n",
        "        super(GATEmbeddingBlock, self).__init__()\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        # GAT layer to process the sector embeddings\n",
        "        self.gat = GATConv(input_dim, input_dim, heads=num_heads, concat=False)\n",
        "\n",
        "    def forward(self, sector_embeddings, edge_index):\n",
        "        \"\"\"\n",
        "        sector_embeddings: Tensor of shape (num_sectors, input_dim) representing the sector embeddings.\n",
        "        edge_index: The graph structure (edge indices) for GAT.\n",
        "\n",
        "        Returns:\n",
        "        - learned_sector_embeddings: Tensor of shape (num_sectors, input_dim) representing the updated sector embeddings.\n",
        "        \"\"\"\n",
        "        # Apply GAT\n",
        "        learned_sector_embeddings = self.gat(sector_embeddings, edge_index)\n",
        "\n",
        "        return learned_sector_embeddings\n",
        "\n",
        "# # Example usage\n",
        "# batch_size = 403\n",
        "# t = 6  # Look-back period for long-term embedding\n",
        "# input_dim = 64\n",
        "# num_sectors = 5  # Number of sectors\n",
        "\n",
        "# # Simulate the gi_batch (sector-level) embeddings\n",
        "# gi_batch = torch.randn(batch_size, input_dim)\n",
        "# # Simulate the stock names for each embedding\n",
        "# batch_names = [f\"stock_{i}\" for i in range(batch_size)]\n",
        "\n",
        "# # Simulate a sector dictionary with some dummy data\n",
        "# sector_dict = {\n",
        "#     \"sector_1\": [f\"stock_{i}\" for i in range(0, 100)],\n",
        "#     \"sector_2\": [f\"stock_{i}\" for i in range(100, 200)],\n",
        "#     \"sector_3\": [f\"stock_{i}\" for i in range(200, 300)],\n",
        "#     \"sector_4\": [f\"stock_{i}\" for i in range(300, 400)],\n",
        "#     \"sector_5\": [f\"stock_{i}\" for i in range(400, 403)]\n",
        "# }\n",
        "\n",
        "# # Initialize the sector embedding block\n",
        "# sector_embedding_block = SectorEmbeddingBlock(input_dim=input_dim, num_sectors=num_sectors, t=t)\n",
        "\n",
        "# # Get the sector-level embeddings\n",
        "# sector_embeddings = sector_embedding_block(gi_batch, batch_names, sector_dict)\n",
        "\n",
        "# # Construct a fully connected graph for sectors\n",
        "# # Fully connected graph means every sector is connected to every other sector\n",
        "# edge_index = torch.combinations(torch.arange(num_sectors), r=2).T  # (2, num_edges)\n",
        "\n",
        "# # Initialize the GAT embedding block\n",
        "# gat_embedding_block = GATEmbeddingBlock(input_dim=input_dim, num_sectors=num_sectors, num_heads=1)\n",
        "\n",
        "# # Get the learned sector embeddings after applying GAT\n",
        "# learned_sector_embeddings = gat_embedding_block(sector_embeddings, edge_index)\n",
        "\n",
        "# print(\"Learned sector embeddings:\", learned_sector_embeddings.shape)  # (num_sectors, input_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "e9fpw1XjcvZy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GATConv\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "# Function to map stocks to sector embeddings and concatenate them\n",
        "def map_stocks_to_sector_embeddings(batch_names, sector_dict, sector_embeddings, input_dim, stock_embeddings):\n",
        "    \"\"\"\n",
        "    Map stocks in `batch_names` to their sector embeddings based on `sector_dict`.\n",
        "    Then, concatenate stock-level, sector-level, and inter-sector embeddings.\n",
        "\n",
        "    - batch_names: List of stock names in the order they appear in the embeddings.\n",
        "    - sector_dict: Dictionary mapping sector names to a list of stock names in that sector.\n",
        "    - sector_embeddings: Tensor of sector embeddings (num_sectors, input_dim).\n",
        "    - input_dim: The embedding size (dimensionality of each embedding vector).\n",
        "\n",
        "    Returns:\n",
        "    - final_embeddings: Tensor containing the concatenated embeddings for each stock.\n",
        "    \"\"\"\n",
        "    batch_size = len(batch_names)\n",
        "    #print(\"batch size is\", batch_size)\n",
        "    sector_map = {name: sector for sector, names in sector_dict.items() for name in names}\n",
        "    # Create a tensor to hold the mapped inter-sector embeddings for each stock\n",
        "    mapped_inter_sector_embeddings = torch.ones(batch_size, input_dim, device=stock_embeddings.device)\n",
        "    # Iterate over batch_names and assign sector embeddings\n",
        "    for i, stock in enumerate(batch_names):\n",
        "        if stock not in sector_map.keys():\n",
        "            #print(stock,\" not in sector_map\")\n",
        "            continue\n",
        "        sector_name = sector_map[stock]  # Find the sector of stock i\n",
        "        sector_id = list(sector_dict.keys()).index(sector_name)  # Get sector index\n",
        "        mapped_inter_sector_embeddings[i] = sector_embeddings[sector_id]  # Map sector embedding\n",
        "    # print(\"Stcok embeddgins\", stock_embeddings.shape)\n",
        "    # print(\"mapped_inter_sector_embeddings shape\",mapped_inter_sector_embeddings.shape)\n",
        "    # Concatenate stock-level, sector-level, and inter-sector embeddings\n",
        "    return mapped_inter_sector_embeddings\n",
        "\n",
        "# Define the GAT block for sector embeddings processing\n",
        "class GATEmbeddingBlock(nn.Module):\n",
        "    def __init__(self, input_dim, num_sectors, num_heads=1):\n",
        "        super(GATEmbeddingBlock, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.gat = GATConv(input_dim, input_dim, heads=num_heads, concat=False)\n",
        "\n",
        "    def forward(self, sector_embeddings, edge_index):\n",
        "        learned_sector_embeddings = self.gat(sector_embeddings, edge_index)\n",
        "        return learned_sector_embeddings\n",
        "\n",
        "# Example usage\n",
        "# batch_size = 407\n",
        "# input_dim = 64\n",
        "# num_sectors = 5  # Number of sectors\n",
        "# t = 6  # Look-back period for long-term embedding\n",
        "\n",
        "# # Simulate stock-level embeddings (gi_batch)\n",
        "# stock_embeddings = torch.randn(batch_size, input_dim)  # (403, 64) for stock-specific\n",
        "# # Simulate sector-level embeddings (sector_embeddings)\n",
        "# sector_embeddings = torch.randn(num_sectors, input_dim)  # (5, 64) for sector-level embeddings\n",
        "\n",
        "# # Simulate batch_names (list of stock names)\n",
        "# batch_names = [f\"stock_{i}\" for i in range(batch_size)]\n",
        "\n",
        "# # Simulate sector dictionary (mapping sectors to stock names)\n",
        "# sector_dict = {\n",
        "#     \"sector_1\": [f\"stock_{i}\" for i in range(0, 100)],\n",
        "#     \"sector_2\": [f\"stock_{i}\" for i in range(100, 200)],\n",
        "#     \"sector_3\": [f\"stock_{i}\" for i in range(200, 300)],\n",
        "#     \"sector_4\": [f\"stock_{i}\" for i in range(300, 400)],\n",
        "#     \"sector_5\": [f\"stock_{i}\" for i in range(400, 403)],\n",
        "# }\n",
        "\n",
        "# # Call the function to map stocks to sector embeddings and concatenate them\n",
        "# final_embeddings = map_stocks_to_sector_embeddings(batch_names, sector_dict, sector_embeddings, input_dim)\n",
        "\n",
        "# # Construct a fully connected graph for sectors\n",
        "# edge_index = torch.combinations(torch.arange(num_sectors), r=2).T  # (2, num_edges)\n",
        "\n",
        "# # Initialize the GAT embedding block\n",
        "# gat_embedding_block = GATEmbeddingBlock(input_dim=input_dim, num_sectors=num_sectors, num_heads=1)\n",
        "\n",
        "# # Get the learned sector embeddings after applying GAT\n",
        "# learned_sector_embeddings = gat_embedding_block(sector_embeddings, edge_index)\n",
        "\n",
        "# print(\"Final concatenated embeddings shape:\", final_embeddings.shape)  # Should be (403, 128)\n",
        "# print(\"Learned sector embeddings shape:\", learned_sector_embeddings.shape)  # (num_sectors, input_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "EB0VDUG2i4G6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class EmbeddingFusionLayer(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        \"\"\"\n",
        "        The Embedding Fusion Layer that combines short-term, intra-sector, and inter-sector embeddings.\n",
        "\n",
        "        - input_dim: The dimensionality of the individual embeddings (e.g., 64).\n",
        "        \"\"\"\n",
        "        super(EmbeddingFusionLayer, self).__init__()\n",
        "\n",
        "        # Learnable weight matrix for fusion\n",
        "        self.Wf = nn.Linear(input_dim * 3, input_dim)  # Input is concatenated, so 3 * input_dim\n",
        "\n",
        "    def forward(self, short_term_embeddings, intra_sector_embeddings, inter_sector_embeddings_mapped):\n",
        "        \"\"\"\n",
        "        Forward pass to combine the embeddings.\n",
        "\n",
        "        - short_term_embeddings: Tensor of shape (batch_size, input_dim)\n",
        "        - intra_sector_embeddings: Tensor of shape (batch_size, input_dim)\n",
        "        - inter_sector_embeddings: Tensor of shape (batch_size, input_dim)\n",
        "\n",
        "        Returns:\n",
        "        - final_embeddings: Tensor of shape (batch_size, input_dim)\n",
        "        \"\"\"\n",
        "        # Concatenate the embeddings along the feature dimension (axis 1)\n",
        "        concatenated_embeddings = torch.cat(\n",
        "            (short_term_embeddings, intra_sector_embeddings, inter_sector_embeddings_mapped), dim=1\n",
        "        )  # Shape: (batch_size, input_dim*3)\n",
        "\n",
        "        # Apply the fusion weight matrix (Wf) and ReLU activation\n",
        "        final_embeddings = F.relu(self.Wf(concatenated_embeddings))\n",
        "\n",
        "        return final_embeddings\n",
        "\n",
        "# # Example usage\n",
        "# batch_size = 403\n",
        "# input_dim = 64\n",
        "\n",
        "# # Simulate the individual embeddings (for simplicity)\n",
        "# short_term_embeddings = torch.randn(batch_size, input_dim)  # (batch_size, input_dim)\n",
        "# intra_sector_embeddings = torch.randn(batch_size, input_dim)  # (batch_size, input_dim)\n",
        "# inter_sector_embeddings_mapped = torch.randn(batch_size, input_dim)  # (batch_size, input_dim)\n",
        "\n",
        "# # Initialize the embedding fusion layer\n",
        "# embedding_fusion_layer = EmbeddingFusionLayer(input_dim=input_dim)\n",
        "\n",
        "# # Get the final fused embeddings\n",
        "# final_embeddings = embedding_fusion_layer(short_term_embeddings, intra_sector_embeddings, inter_sector_embeddings_mapped)\n",
        "\n",
        "# print(\"Final fused embeddings shape:\", final_embeddings.shape)  # Should be (batch_size, input_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "11K1d95Gc02z"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class LossFunction(nn.Module):\n",
        "    def __init__(self, input_dim, return_dim=1, move_dim=1, lambda_reg=0.01):\n",
        "        \"\"\"\n",
        "        Model to predict return ratio and stock movement.\n",
        "\n",
        "        - input_dim: The input dimension (e.g., size of the final fused embedding).\n",
        "        - return_dim: Output dimension for return ratio (typically 1).\n",
        "        - move_dim: Output dimension for movement (typically 1, binary classification).\n",
        "        - lambda_reg: Regularization parameter for L2 regularization.\n",
        "        \"\"\"\n",
        "        super(LossFunction, self).__init__()\n",
        "\n",
        "        # Task-specific parameters\n",
        "        self.e1 = nn.Parameter(torch.randn(input_dim, device=device))  # Hidden vector for return ratio\n",
        "        self.b1 = nn.Parameter(torch.zeros(1, device=device))  # Bias for return ratio\n",
        "\n",
        "        self.e2 = nn.Parameter(torch.randn(input_dim, device=device))  # Hidden vector for stock movement\n",
        "        self.b2 = nn.Parameter(torch.zeros(1, device=device))  # Bias for stock movement\n",
        "\n",
        "        self.lambda_reg = lambda_reg  # Regularization parameter\n",
        "\n",
        "    def get_ys(self, tau_F):\n",
        "        \"\"\"\n",
        "        Forward pass for return and movement prediction.\n",
        "\n",
        "        - tau_F: The final fused embeddings of shape (batch_size, input_dim).\n",
        "\n",
        "        Returns:\n",
        "        - y_return: Predicted return ratio.\n",
        "        - y_move: Predicted stock movement.\n",
        "        \"\"\"\n",
        "        # Calculate return prediction\n",
        "        y_return = torch.matmul(tau_F, self.e1) + self.b1\n",
        "\n",
        "        # Calculate movement prediction (sigmoid for binary classification)\n",
        "        y_move = torch.sigmoid(torch.matmul(tau_F, self.e2) + self.b2)\n",
        "\n",
        "        return y_return, y_move\n",
        "\n",
        "    def compute_rank_loss(self, y_return, y_true_return):\n",
        "        \"\"\"\n",
        "        Compute the pairwise ranking loss (Lrank).\n",
        "\n",
        "        - y_return: Predicted return ratio for each stock.\n",
        "        - y_true_return: Ground truth return ratio for each stock.\n",
        "\n",
        "        Returns:\n",
        "        - ranking_loss: The ranking loss for all pairs of stocks.\n",
        "        \"\"\"\n",
        "        # Vectorized pairwise ranking loss computation\n",
        "        y_diff_pred = y_return.unsqueeze(1) - y_return.unsqueeze(0)  # (N, N) matrix\n",
        "        y_diff_true = y_true_return.unsqueeze(1) - y_true_return.unsqueeze(0)  # (N, N) matrix\n",
        "\n",
        "        mask = (y_diff_true > 0).float()  # Only consider positive ranking differences\n",
        "        Lrank = torch.sum(F.relu(-y_diff_pred * y_diff_true) * mask) / (torch.sum(mask) + 1e-8)  # Avoid division by zero\n",
        "\n",
        "        return Lrank\n",
        "\n",
        "    def compute_move_loss(self, y_move, y_true_move):\n",
        "        \"\"\"\n",
        "        Compute the cross-entropy loss for stock movement prediction (Lmove).\n",
        "\n",
        "        - y_move: Predicted movement (binary classification).\n",
        "        - y_true_move: Ground truth movement (0 or 1).\n",
        "\n",
        "        Returns:\n",
        "        - movement_loss: The cross-entropy loss for stock movement.\n",
        "        \"\"\"\n",
        "        return F.binary_cross_entropy(y_move, y_true_move)\n",
        "\n",
        "    def forward(self, tau_F, y_true_return, y_true_move, delta=0.5):\n",
        "        \"\"\"\n",
        "        Compute the total loss combining ranking loss, movement loss, and L2 regularization.\n",
        "\n",
        "        - tau_F: Final fused embeddings (used in return and movement prediction).\n",
        "        - y_true_return: Ground truth return ratios.\n",
        "        - y_true_move: Ground truth stock movements (binary).\n",
        "        - delta: Weight for ranking vs. movement loss.\n",
        "\n",
        "        Returns:\n",
        "        - total_loss: The final loss value (weighted sum of ranking and movement losses).\n",
        "        \"\"\"\n",
        "        # Predict return and movement\n",
        "        y_return, y_move = self.get_ys(tau_F)\n",
        "\n",
        "        # Compute ranking loss\n",
        "        Lrank = self.compute_rank_loss(y_return, y_true_return)\n",
        "\n",
        "        # Compute movement loss\n",
        "        Lmove = self.compute_move_loss(y_move, y_true_move)\n",
        "\n",
        "        # L2 regularization term\n",
        "        L2_reg = torch.norm(self.e1) ** 2 + torch.norm(self.e2) ** 2\n",
        "\n",
        "        # Total loss\n",
        "        total_loss = (1 - delta) * Lrank + delta * Lmove + self.lambda_reg * L2_reg\n",
        "\n",
        "        return y_return, y_move, total_loss\n",
        "\n",
        "# Example usage\n",
        "batch_size = 403\n",
        "input_dim = 64  # Size of final fused embedding\n",
        "\n",
        "# Simulated final fused embeddings from the previous steps\n",
        "tau_F = torch.randn(batch_size, input_dim, device=device)\n",
        "\n",
        "# Simulated ground truth data for return and movement\n",
        "y_true_return = torch.randn(batch_size, device=device)  # Ground truth return ratios\n",
        "y_true_move = torch.randint(0, 2, (batch_size,), dtype=torch.float32, device=device)  # Ground truth movement (binary)\n",
        "\n",
        "# Initialize the model\n",
        "model = LossFunction(input_dim=input_dim).to(device)\n",
        "\n",
        "# # Compute the total loss\n",
        "# total_loss = model(tau_F, y_true_return, y_true_move)\n",
        "\n",
        "# print(\"Total loss:\", total_loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "O7PHUm0Vc3At"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GATConv\n",
        "\n",
        "class CompleteStockModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_sectors, t=6, heads=4, dropout=0.05):\n",
        "        super(CompleteStockModel, self).__init__()\n",
        "\n",
        "        # Define each block from the given code\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.attentive_gru = AttentiveLSTM(input_dim, hidden_dim)\n",
        "        self.stock_gat = StockGAT(hidden_dim, hidden_dim, hidden_dim, heads, dropout)\n",
        "        self.long_term_attention_block = LongTermAttentionBlock(hidden_dim, hidden_dim, t)\n",
        "        self.sector_embedding_block = SectorEmbeddingBlock(hidden_dim, num_sectors, t)\n",
        "        self.gat_embedding_block = GATEmbeddingBlock(hidden_dim, num_sectors, heads)\n",
        "        self.embedding_fusion_layer = EmbeddingFusionLayer(hidden_dim)\n",
        "\n",
        "    def forward(self, x, batch_stock_names, intra_sector_stock_2_stock, sector_dict):\n",
        "        # Step 1: Compute weekly embeddings using AttentiveGRU\n",
        "        weekly_embeddings = self.attentive_gru(x)  # Shape: (batch_size, num_weeks, hidden_dim)\n",
        "        weekly_embeddings.to(device)\n",
        "        # Step 2: Process the embeddings through the StockGAT model to get sector-level embeddings\n",
        "        gi_batch = process_batch(batch_stock_names, weekly_embeddings, self.stock_gat, intra_sector_stock_2_stock)\n",
        "\n",
        "        # Step 3: Compute long-term trends using LongTermAttentionBlock\n",
        "        long_term_ai, long_term_gi = self.long_term_attention_block(weekly_embeddings, gi_batch)\n",
        "        # print(\"ltai shape\", long_term_ai.shape)\n",
        "        # print(\"ltgi shape\", long_term_gi.shape)\n",
        "\n",
        "        # Step 4: Calculate sector embeddings using SectorEmbeddingBlock\n",
        "        sector_embeddings = self.sector_embedding_block(long_term_gi, batch_stock_names, sector_dict)\n",
        "        # print(\"sector embeddings shape\", sector_embeddings.shape)\n",
        "\n",
        "        # Step 5: Apply GAT on sector embeddings to learn inter-sector relationships\n",
        "        learned_sector_embeddings = self.gat_embedding_block(sector_embeddings, self.create_edge_index(sector_dict))\n",
        "        # print(\"learned embeddings shape\", learned_sector_embeddings.shape)\n",
        "\n",
        "        # Step 6: Map stocks to their corresponding sector embeddings\n",
        "        final_stock_embeddings = map_stocks_to_sector_embeddings(batch_stock_names, sector_dict, learned_sector_embeddings, self.hidden_dim, long_term_gi)\n",
        "        # print(\"final stock embeddings shape\",final_stock_embeddings.shape)\n",
        "\n",
        "        # Step 7: Fuse the embeddings using the EmbeddingFusionLayer\n",
        "        final_embeddings = self.embedding_fusion_layer(long_term_ai, long_term_gi, final_stock_embeddings)\n",
        "        # print(\"final embeddings shape\",final_embeddings.shape)\n",
        "        weekly_embeddings = weekly_embeddings.to(device)\n",
        "        gi_batch = gi_batch.to(device)\n",
        "        long_term_ai = long_term_ai.to(device)\n",
        "        long_term_gi = long_term_gi.to(device)\n",
        "        sector_embeddings = sector_embeddings.to(device)\n",
        "        learned_sector_embeddings = learned_sector_embeddings.to(device)\n",
        "        final_stock_embeddings = final_stock_embeddings.to(device)\n",
        "        final_embeddings = final_embeddings.to(device)\n",
        "\n",
        "        return final_embeddings\n",
        "\n",
        "    def create_edge_index(self, sector_dict):\n",
        "        \"\"\"\n",
        "        Creates an edge index for the GAT embedding block based on the sectors.\n",
        "        \"\"\"\n",
        "        edge_index_list = []\n",
        "        sector_names = list(sector_dict.keys())\n",
        "\n",
        "        for i, sector in enumerate(sector_names):\n",
        "            for j in range(i + 1, len(sector_names)):  # Connect all sectors to each other\n",
        "                edge_index_list.append([i, j])\n",
        "                edge_index_list.append([j, i])\n",
        "\n",
        "        edge_index = torch.tensor(edge_index_list, dtype=torch.long).t().contiguous().to(device)\n",
        "        return edge_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "efXOVPi6QT_Z"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Dictionaries to store stock relationships\n",
        "intra_sector_stock_2_stock = {}\n",
        "sector_stocks = {}\n",
        "\n",
        "file_path = 'Preprocessed_data'\n",
        "for file in os.listdir(file_path):\n",
        "    df = pd.read_csv(file_path + \"/\" + file)\n",
        "    sector = df['Sector_Encoded'][0]\n",
        "    if sector in sector_stocks:\n",
        "        sector_stocks[sector].append(file[:-9])\n",
        "    else:\n",
        "        sector_stocks[sector] = [file[:-9]]\n",
        "sector_stocks = {k: v for k, v in sector_stocks.items() if len(v) > 1}\n",
        "\n",
        "name_matrix = {}\n",
        "for same_sector_stocks in sector_stocks.values():\n",
        "    for stock_key in same_sector_stocks:\n",
        "        name_matrix.setdefault(stock_key, []).extend(\n",
        "            [stock_value for stock_value in same_sector_stocks if stock_value != stock_key]\n",
        "        )\n",
        "\n",
        "intra_sector_stock_2_stock = name_matrix\n",
        "\n",
        "inter_sector_matrix = {sector: [other_sector for other_sector in sector_stocks.keys() if sector != other_sector] for sector in sector_stocks.keys()}\n",
        "\n",
        "# # Create a graph\n",
        "# G = nx.Graph()\n",
        "\n",
        "# # Add intra-sector stock relationships\n",
        "# for stock, related_stocks in intra_sector_stock_2_stock.items():\n",
        "#     for related_stock in related_stocks:\n",
        "#         G.add_edge(stock, related_stock, color='blue')\n",
        "\n",
        "# # Add inter-sector relationships\n",
        "# for sector, related_sectors in inter_sector_matrix.items():\n",
        "#     for related_sector in related_sectors:\n",
        "#         G.add_edge(sector, related_sector, color='red')\n",
        "\n",
        "# # Draw the graph with better aesthetics\n",
        "# plt.figure(figsize=(14, 10))\n",
        "# edges = G.edges()\n",
        "# colors = [G[u][v]['color'] for u, v in edges]\n",
        "# pos = nx.spring_layout(G, k=0.3)  # Adjust spacing for clarity\n",
        "# nx.draw(G, pos, with_labels=False, edge_color=colors, node_size=400, node_color='lightblue', edgecolors='black')\n",
        "# plt.title(\"Stock Intra-sector and Inter-sector Relationships\", fontsize=14, fontweight='bold')\n",
        "# plt.show()*/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "QCQQD9F3dNZT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2bc6221-c831-4a26-de9b-fbe5246d9ba1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output shape cuda:0\n"
          ]
        }
      ],
      "source": [
        "model = CompleteStockModel(input_dim=15, hidden_dim=64, num_sectors=23).to(device)\n",
        "\n",
        "data_dir = '/content/Preprocessed_data'  # Directory containing 403 CSV files\n",
        "window_size = 30  # Number of timesteps per batch for each stock\n",
        "\n",
        "# Initialize the data loader\n",
        "data_loader = StockDataLoader(data_dir, window_size)\n",
        "\n",
        "# Create an iterator\n",
        "data_iter = iter(data_loader)\n",
        "\n",
        "x, stock_names, y1, y2 = next(data_iter)\n",
        "\n",
        "# Move tensors to the device\n",
        "x = x.to(device)\n",
        "y1, y2 = y1.to(device), y2.to(device)\n",
        "\n",
        "# If intra_sector_stock_2_stock and sector_stocks are tensors, move them too\n",
        "#intra_sector_stock_2_stock = intra_sector_stock_2_stock.to(device)\n",
        "#sector_stocks = {key: val.to(device) for key, val in sector_stocks.items()}  # If sector_stocks is a dictionary of tensors\n",
        "#print(sector_stocks)\n",
        "# Pass everything to the model\n",
        "output = model(x, stock_names, intra_sector_stock_2_stock, sector_stocks)\n",
        "print(\"output shape\", output.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "3DSAbOpIQxSp"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "loss_fn = LossFunction(64)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "1LlWsZBydUlh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Constants and clipping bounds\n",
        "RETURN_CLIP_MIN = -0.1\n",
        "RETURN_CLIP_MAX = 0.1\n",
        "λ = 0.5  # Weight for movement prediction loss\n",
        "\n",
        "# Hyperparameters\n",
        "epoch_list = [10, 20, 30]\n",
        "batch_list = [4, 8, 16]\n",
        "top_k_values = [5, 10, 20]\n",
        "\n",
        "# Best tracking\n",
        "best_predicted = {}\n",
        "best_actual = {}\n",
        "max_mrr = 0\n",
        "best_mse = best_mae = best_movement_accuracy = 0\n",
        "maxbatch = 0\n",
        "max_epochs = 0\n",
        "\n",
        "# Loss functions\n",
        "loss_fn_return = torch.nn.SmoothL1Loss()\n",
        "loss_fn_move = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Assuming 'dataset' is your complete dataset\n",
        "total_size = len(x)\n",
        "train_size = int(0.8 * total_size)\n",
        "val_size = total_size - train_size\n",
        "train_dataset, val_dataset = random_split(x, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Initialize model, optimizer, scheduler\n",
        "model = CompleteStockModel(input_dim=15, hidden_dim=64, num_sectors=23).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RLReranker(torch.nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(RLReranker, self).__init__()\n",
        "        self.linear = torch.nn.Linear(input_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "# Initialize RL reranker\n",
        "rl_reranker = RLReranker(input_dim=1).to(device)\n",
        "rl_optimizer = torch.optim.Adam(rl_reranker.parameters(), lr=1e-4)\n"
      ],
      "metadata": {
        "id": "sVmblcYIbCWZ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# --- Hyperparameters and constants ---\n",
        "RETURN_CLIP_MIN = -0.1\n",
        "RETURN_CLIP_MAX = 0.1\n",
        "λ = 0.5  # Weight for movement loss\n",
        "\n",
        "epoch_list = [10, 20, 30]\n",
        "batch_list = [4, 8, 16]\n",
        "top_k_values = [5, 10, 20]\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss functions and optimizer\n",
        "loss_fn_return = torch.nn.SmoothL1Loss()\n",
        "loss_fn_move = torch.nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "\n",
        "# --- Best tracking ---\n",
        "max_mrr = 0\n",
        "best_predicted = {}\n",
        "best_actual = {}\n",
        "best_mse = best_mae = best_movement_accuracy = 0\n",
        "maxbatch = 0\n",
        "max_epochs = 0\n",
        "\n",
        "for num_epochs in epoch_list:\n",
        "    for max_batches in batch_list:\n",
        "        for epoch in range(num_epochs):\n",
        "            model.train()\n",
        "            running_loss = 0.0\n",
        "            batch_count = 0\n",
        "            stock_return_dict = {}\n",
        "            stock_true_return_dict = {}\n",
        "\n",
        "            data_loader = StockDataLoader(data_dir, window_size)\n",
        "            data_iter = iter(data_loader)\n",
        "\n",
        "            for batch_idx in range(max_batches):\n",
        "                try:\n",
        "                    x, stock_names, y_true_move, y_true_return = next(data_iter)\n",
        "                except StopIteration:\n",
        "                    print(\"End of data reached.\")\n",
        "                    break\n",
        "\n",
        "                x = x.to(device)\n",
        "                y_true_move = y_true_move.to(device).view(-1)\n",
        "                y_true_return = y_true_return.to(device).view(-1)\n",
        "                y_true_return = torch.clamp(y_true_return, min=RETURN_CLIP_MIN, max=RETURN_CLIP_MAX)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # --- Model forward ---\n",
        "                y_return_pred, y_move_pred = model(x, stock_names, intra_sector_stock_2_stock, sector_stocks)[:2]\n",
        "                y_return_pred = torch.clamp(y_return_pred.view(-1), min=RETURN_CLIP_MIN, max=RETURN_CLIP_MAX)\n",
        "                y_move_pred = y_move_pred.view(-1)\n",
        "\n",
        "                # Ensure sizes match\n",
        "                min_len = min(y_return_pred.shape[0], y_true_return.shape[0])\n",
        "                y_return_pred = y_return_pred[:min_len]\n",
        "                y_true_return = y_true_return[:min_len]\n",
        "\n",
        "                min_len_move = min(y_move_pred.shape[0], y_true_move.shape[0])\n",
        "                y_move_pred = y_move_pred[:min_len_move]\n",
        "                y_true_move = y_true_move[:min_len_move]\n",
        "\n",
        "                # --- Loss ---\n",
        "                loss_return = loss_fn_return(y_return_pred, y_true_return)\n",
        "                loss_move = loss_fn_move(y_move_pred, y_true_move.float())\n",
        "                loss = loss_return + λ * loss_move\n",
        "\n",
        "                if torch.isnan(loss):\n",
        "                    print(f\"NaN in loss at Epoch {epoch + 1}, Batch {batch_idx + 1}\")\n",
        "                    break\n",
        "\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "\n",
        "                running_loss += loss.detach().cpu().item()\n",
        "                batch_count += 1\n",
        "\n",
        "                for i, stock in enumerate(stock_names[:min_len]):\n",
        "                    stock_return_dict[stock] = y_return_pred[i].detach().cpu().item()\n",
        "                    stock_true_return_dict[stock] = y_true_return[i].detach().cpu().item()\n",
        "\n",
        "            if batch_count == 0:\n",
        "                print(\"No batches processed, exiting training.\")\n",
        "                exit()\n",
        "\n",
        "            avg_loss = running_loss / batch_count\n",
        "            scheduler.step()\n",
        "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Avg Loss: {avg_loss:.6f}\")\n",
        "\n",
        "            # --- Evaluation ---\n",
        "            sorted_predicted = sorted(stock_return_dict.items(), key=lambda item: item[1], reverse=True)\n",
        "            sorted_actual = sorted(stock_true_return_dict.items(), key=lambda item: item[1], reverse=True)\n",
        "            predicted_ranks = {stock: rank + 1 for rank, (stock, _) in enumerate(sorted_predicted)}\n",
        "            actual_ranks = {stock: rank + 1 for rank, (stock, _) in enumerate(sorted_actual)}\n",
        "\n",
        "            sum_k = 0\n",
        "            for k in top_k_values:\n",
        "                top_k_predicted_stocks = sorted_predicted[:k]\n",
        "                mrr_top_k = sum(1.0 / actual_ranks[stock] for stock, _ in top_k_predicted_stocks)\n",
        "                sum_k += mrr_top_k / k\n",
        "\n",
        "            predicted_returns = list(stock_return_dict.values())\n",
        "            true_returns = list(stock_true_return_dict.values())\n",
        "            predicted_moves = [int(p > 0) for p in predicted_returns]\n",
        "            true_moves = [int(t > 0) for t in true_returns]\n",
        "\n",
        "            mae = mean_absolute_error(true_returns, predicted_returns)\n",
        "            mse = mean_squared_error(true_returns, predicted_returns)\n",
        "            movement_accuracy = sum(p == t for p, t in zip(predicted_moves, true_moves)) / len(true_moves)\n",
        "\n",
        "            print(f\"MAE: {mae:.6f}, MSE: {mse:.6f}, Move Acc: {movement_accuracy:.4f}, MRR sum_k: {sum_k:.4f}\")\n",
        "\n",
        "            if sum_k > max_mrr:\n",
        "                max_mrr = sum_k\n",
        "                maxbatch = max_batches\n",
        "                max_epochs = num_epochs\n",
        "                best_predicted = sorted_predicted\n",
        "                best_actual = sorted_actual\n",
        "                best_mse = mse\n",
        "                best_mae = mae\n",
        "                best_movement_accuracy = movement_accuracy\n",
        "\n",
        "print(\"\\n\\U0001F50D Best Results:\")\n",
        "print(f\"Best MAE: {best_mae:.6f}\")\n",
        "print(f\"Best MSE: {best_mse:.6f}\")\n",
        "print(f\"Best Movement Accuracy: {best_movement_accuracy:.4f}\")\n",
        "print(f\"Best Epoch Count: {max_epochs}, Best Batch Count: {maxbatch}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVcD-AnCbIc4",
        "outputId": "f8510b4d-ed5e-4c7f-f8c5-7921f751c79b"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Avg Loss: 0.351203\n",
            "MAE: 0.041035, MSE: 0.003172, Move Acc: 0.5781, MRR sum_k: 0.4165\n",
            "Epoch [2/10], Avg Loss: 0.348880\n",
            "MAE: 0.038623, MSE: 0.003229, Move Acc: 0.5469, MRR sum_k: 0.2491\n",
            "Epoch [3/10], Avg Loss: 0.351462\n",
            "MAE: 0.036923, MSE: 0.003196, Move Acc: 0.5000, MRR sum_k: 0.1802\n",
            "Epoch [4/10], Avg Loss: 0.342745\n",
            "MAE: 0.032966, MSE: 0.002443, Move Acc: 0.5625, MRR sum_k: 0.2238\n",
            "Epoch [5/10], Avg Loss: 0.349290\n",
            "MAE: 0.028578, MSE: 0.002109, Move Acc: 0.5312, MRR sum_k: 0.1426\n",
            "Epoch [6/10], Avg Loss: 0.348544\n",
            "MAE: 0.030740, MSE: 0.002106, Move Acc: 0.5625, MRR sum_k: 0.1963\n",
            "Epoch [7/10], Avg Loss: 0.351724\n",
            "MAE: 0.032372, MSE: 0.002264, Move Acc: 0.4375, MRR sum_k: 0.1153\n",
            "Epoch [8/10], Avg Loss: 0.349334\n",
            "MAE: 0.031834, MSE: 0.002372, Move Acc: 0.4375, MRR sum_k: 0.1305\n",
            "Epoch [9/10], Avg Loss: 0.345582\n",
            "MAE: 0.028759, MSE: 0.001819, Move Acc: 0.5469, MRR sum_k: 0.2959\n",
            "Epoch [10/10], Avg Loss: 0.344691\n",
            "MAE: 0.033403, MSE: 0.002335, Move Acc: 0.5469, MRR sum_k: 0.1029\n",
            "Epoch [1/10], Avg Loss: 0.349471\n",
            "MAE: 0.026316, MSE: 0.001814, Move Acc: 0.5000, MRR sum_k: 0.1794\n",
            "Epoch [2/10], Avg Loss: 0.349322\n",
            "MAE: 0.025836, MSE: 0.001554, Move Acc: 0.4844, MRR sum_k: 0.3330\n",
            "Epoch [3/10], Avg Loss: 0.349991\n",
            "MAE: 0.025929, MSE: 0.001569, Move Acc: 0.5312, MRR sum_k: 0.2190\n",
            "Epoch [4/10], Avg Loss: 0.348403\n",
            "MAE: 0.026683, MSE: 0.001841, Move Acc: 0.5156, MRR sum_k: 0.1429\n",
            "Epoch [5/10], Avg Loss: 0.346785\n",
            "MAE: 0.023249, MSE: 0.001457, Move Acc: 0.5000, MRR sum_k: 0.1175\n",
            "Epoch [6/10], Avg Loss: 0.347599\n",
            "MAE: 0.023647, MSE: 0.001342, Move Acc: 0.4531, MRR sum_k: 0.2178\n",
            "Epoch [7/10], Avg Loss: 0.346306\n",
            "MAE: 0.022295, MSE: 0.001433, Move Acc: 0.4531, MRR sum_k: 0.1562\n",
            "Epoch [8/10], Avg Loss: 0.348064\n",
            "MAE: 0.021031, MSE: 0.001103, Move Acc: 0.5469, MRR sum_k: 0.2204\n",
            "Epoch [9/10], Avg Loss: 0.347324\n",
            "MAE: 0.023952, MSE: 0.001458, Move Acc: 0.5000, MRR sum_k: 0.1677\n",
            "Epoch [10/10], Avg Loss: 0.346735\n",
            "MAE: 0.017705, MSE: 0.000807, Move Acc: 0.5469, MRR sum_k: 0.2703\n",
            "Epoch [1/10], Avg Loss: 0.347428\n",
            "MAE: 0.022516, MSE: 0.001168, Move Acc: 0.4844, MRR sum_k: 0.1748\n",
            "Epoch [2/10], Avg Loss: 0.346944\n",
            "MAE: 0.024584, MSE: 0.001414, Move Acc: 0.5781, MRR sum_k: 0.1399\n",
            "Epoch [3/10], Avg Loss: 0.347213\n",
            "MAE: 0.018951, MSE: 0.000863, Move Acc: 0.4375, MRR sum_k: 0.1784\n",
            "Epoch [4/10], Avg Loss: 0.347407\n",
            "MAE: 0.018077, MSE: 0.000765, Move Acc: 0.4219, MRR sum_k: 0.1671\n",
            "Epoch [5/10], Avg Loss: 0.346505\n",
            "MAE: 0.017387, MSE: 0.000778, Move Acc: 0.3750, MRR sum_k: 0.1067\n",
            "Epoch [6/10], Avg Loss: 0.345171\n",
            "MAE: 0.021335, MSE: 0.001031, Move Acc: 0.5625, MRR sum_k: 0.1190\n",
            "Epoch [7/10], Avg Loss: 0.347926\n",
            "MAE: 0.017391, MSE: 0.000810, Move Acc: 0.6406, MRR sum_k: 0.3639\n",
            "Epoch [8/10], Avg Loss: 0.347532\n",
            "MAE: 0.017096, MSE: 0.000743, Move Acc: 0.6406, MRR sum_k: 0.1548\n",
            "Epoch [9/10], Avg Loss: 0.346963\n",
            "MAE: 0.018649, MSE: 0.000860, Move Acc: 0.4688, MRR sum_k: 0.1768\n",
            "Epoch [10/10], Avg Loss: 0.347171\n",
            "MAE: 0.016950, MSE: 0.000671, Move Acc: 0.5156, MRR sum_k: 0.2023\n",
            "Epoch [1/20], Avg Loss: 0.347067\n",
            "MAE: 0.013296, MSE: 0.000438, Move Acc: 0.4219, MRR sum_k: 0.2846\n",
            "Epoch [2/20], Avg Loss: 0.346834\n",
            "MAE: 0.017179, MSE: 0.000690, Move Acc: 0.4688, MRR sum_k: 0.1934\n",
            "Epoch [3/20], Avg Loss: 0.346075\n",
            "MAE: 0.019609, MSE: 0.000917, Move Acc: 0.5156, MRR sum_k: 0.1785\n",
            "Epoch [4/20], Avg Loss: 0.346449\n",
            "MAE: 0.018769, MSE: 0.000816, Move Acc: 0.4844, MRR sum_k: 0.1157\n",
            "Epoch [5/20], Avg Loss: 0.347177\n",
            "MAE: 0.020835, MSE: 0.001141, Move Acc: 0.4375, MRR sum_k: 0.2890\n",
            "Epoch [6/20], Avg Loss: 0.346677\n",
            "MAE: 0.016968, MSE: 0.000671, Move Acc: 0.6094, MRR sum_k: 0.2111\n",
            "Epoch [7/20], Avg Loss: 0.346260\n",
            "MAE: 0.018019, MSE: 0.000985, Move Acc: 0.5000, MRR sum_k: 0.2146\n",
            "Epoch [8/20], Avg Loss: 0.346047\n",
            "MAE: 0.016672, MSE: 0.000625, Move Acc: 0.4844, MRR sum_k: 0.2856\n",
            "Epoch [9/20], Avg Loss: 0.346327\n",
            "MAE: 0.017462, MSE: 0.000799, Move Acc: 0.4844, MRR sum_k: 0.1110\n",
            "Epoch [10/20], Avg Loss: 0.347198\n",
            "MAE: 0.018557, MSE: 0.000855, Move Acc: 0.5312, MRR sum_k: 0.2085\n",
            "Epoch [11/20], Avg Loss: 0.345504\n",
            "MAE: 0.018154, MSE: 0.000689, Move Acc: 0.5781, MRR sum_k: 0.1633\n",
            "Epoch [12/20], Avg Loss: 0.347126\n",
            "MAE: 0.017516, MSE: 0.000737, Move Acc: 0.4219, MRR sum_k: 0.1482\n",
            "Epoch [13/20], Avg Loss: 0.347344\n",
            "MAE: 0.014543, MSE: 0.000445, Move Acc: 0.3906, MRR sum_k: 0.2429\n",
            "Epoch [14/20], Avg Loss: 0.345962\n",
            "MAE: 0.016413, MSE: 0.000616, Move Acc: 0.5312, MRR sum_k: 0.1669\n",
            "Epoch [15/20], Avg Loss: 0.347872\n",
            "MAE: 0.020126, MSE: 0.000850, Move Acc: 0.4688, MRR sum_k: 0.1584\n",
            "Epoch [16/20], Avg Loss: 0.346498\n",
            "MAE: 0.019959, MSE: 0.001008, Move Acc: 0.4531, MRR sum_k: 0.1698\n",
            "Epoch [17/20], Avg Loss: 0.346505\n",
            "MAE: 0.018474, MSE: 0.000799, Move Acc: 0.5000, MRR sum_k: 0.1711\n",
            "Epoch [18/20], Avg Loss: 0.347904\n",
            "MAE: 0.018870, MSE: 0.000957, Move Acc: 0.5781, MRR sum_k: 0.1347\n",
            "Epoch [19/20], Avg Loss: 0.347654\n",
            "MAE: 0.018342, MSE: 0.000719, Move Acc: 0.5312, MRR sum_k: 0.2481\n",
            "Epoch [20/20], Avg Loss: 0.347580\n",
            "MAE: 0.020511, MSE: 0.001124, Move Acc: 0.5625, MRR sum_k: 0.0902\n",
            "Epoch [1/20], Avg Loss: 0.347488\n",
            "MAE: 0.019741, MSE: 0.000756, Move Acc: 0.5000, MRR sum_k: 0.1310\n",
            "Epoch [2/20], Avg Loss: 0.346896\n",
            "MAE: 0.018950, MSE: 0.000760, Move Acc: 0.5469, MRR sum_k: 0.1725\n",
            "Epoch [3/20], Avg Loss: 0.346205\n",
            "MAE: 0.018038, MSE: 0.000779, Move Acc: 0.5625, MRR sum_k: 0.1538\n",
            "Epoch [4/20], Avg Loss: 0.346906\n",
            "MAE: 0.018340, MSE: 0.000756, Move Acc: 0.5156, MRR sum_k: 0.1744\n",
            "Epoch [5/20], Avg Loss: 0.347466\n",
            "MAE: 0.018223, MSE: 0.000797, Move Acc: 0.5156, MRR sum_k: 0.1165\n",
            "Epoch [6/20], Avg Loss: 0.345909\n",
            "MAE: 0.021394, MSE: 0.001019, Move Acc: 0.4688, MRR sum_k: 0.1006\n",
            "Epoch [7/20], Avg Loss: 0.347071\n",
            "MAE: 0.014783, MSE: 0.000538, Move Acc: 0.3125, MRR sum_k: 0.1474\n",
            "Epoch [8/20], Avg Loss: 0.346039\n",
            "MAE: 0.019376, MSE: 0.000854, Move Acc: 0.6094, MRR sum_k: 0.2853\n",
            "Epoch [9/20], Avg Loss: 0.347314\n",
            "MAE: 0.017694, MSE: 0.000655, Move Acc: 0.5469, MRR sum_k: 0.2019\n",
            "Epoch [10/20], Avg Loss: 0.346500\n",
            "MAE: 0.017992, MSE: 0.000778, Move Acc: 0.4844, MRR sum_k: 0.1589\n",
            "Epoch [11/20], Avg Loss: 0.347384\n",
            "MAE: 0.020170, MSE: 0.000846, Move Acc: 0.5000, MRR sum_k: 0.1536\n",
            "Epoch [12/20], Avg Loss: 0.346135\n",
            "MAE: 0.018044, MSE: 0.000806, Move Acc: 0.5156, MRR sum_k: 0.2170\n",
            "Epoch [13/20], Avg Loss: 0.346549\n",
            "MAE: 0.015582, MSE: 0.000577, Move Acc: 0.4219, MRR sum_k: 0.2135\n",
            "Epoch [14/20], Avg Loss: 0.347319\n",
            "MAE: 0.016449, MSE: 0.000603, Move Acc: 0.4844, MRR sum_k: 0.1966\n",
            "Epoch [15/20], Avg Loss: 0.347071\n",
            "MAE: 0.016948, MSE: 0.000642, Move Acc: 0.5156, MRR sum_k: 0.2045\n",
            "Epoch [16/20], Avg Loss: 0.347850\n",
            "MAE: 0.019923, MSE: 0.000802, Move Acc: 0.5156, MRR sum_k: 0.1731\n",
            "Epoch [17/20], Avg Loss: 0.347129\n",
            "MAE: 0.022531, MSE: 0.000967, Move Acc: 0.5469, MRR sum_k: 0.2103\n",
            "Epoch [18/20], Avg Loss: 0.346637\n",
            "MAE: 0.017040, MSE: 0.000635, Move Acc: 0.5156, MRR sum_k: 0.1825\n",
            "Epoch [19/20], Avg Loss: 0.347364\n",
            "MAE: 0.018791, MSE: 0.000877, Move Acc: 0.4688, MRR sum_k: 0.3584\n",
            "Epoch [20/20], Avg Loss: 0.347116\n",
            "MAE: 0.020645, MSE: 0.000800, Move Acc: 0.3594, MRR sum_k: 0.1989\n",
            "Epoch [1/20], Avg Loss: 0.346923\n",
            "MAE: 0.019713, MSE: 0.000873, Move Acc: 0.5469, MRR sum_k: 0.1269\n",
            "Epoch [2/20], Avg Loss: 0.347020\n",
            "MAE: 0.016278, MSE: 0.000719, Move Acc: 0.5469, MRR sum_k: 0.2379\n",
            "Epoch [3/20], Avg Loss: 0.346714\n",
            "MAE: 0.020145, MSE: 0.000911, Move Acc: 0.5469, MRR sum_k: 0.1154\n",
            "Epoch [4/20], Avg Loss: 0.347975\n",
            "MAE: 0.015385, MSE: 0.000652, Move Acc: 0.4688, MRR sum_k: 0.1872\n",
            "Epoch [5/20], Avg Loss: 0.347381\n",
            "MAE: 0.015795, MSE: 0.000677, Move Acc: 0.4375, MRR sum_k: 0.1764\n",
            "Epoch [6/20], Avg Loss: 0.346902\n",
            "MAE: 0.019672, MSE: 0.000949, Move Acc: 0.4219, MRR sum_k: 0.2375\n",
            "Epoch [7/20], Avg Loss: 0.346968\n",
            "MAE: 0.019258, MSE: 0.000823, Move Acc: 0.5000, MRR sum_k: 0.2611\n",
            "Epoch [8/20], Avg Loss: 0.347019\n",
            "MAE: 0.019676, MSE: 0.000880, Move Acc: 0.5469, MRR sum_k: 0.1734\n",
            "Epoch [9/20], Avg Loss: 0.347140\n",
            "MAE: 0.020011, MSE: 0.000885, Move Acc: 0.5156, MRR sum_k: 0.1627\n",
            "Epoch [10/20], Avg Loss: 0.347075\n",
            "MAE: 0.017522, MSE: 0.000838, Move Acc: 0.4375, MRR sum_k: 0.1624\n",
            "Epoch [11/20], Avg Loss: 0.347025\n",
            "MAE: 0.017760, MSE: 0.000705, Move Acc: 0.5469, MRR sum_k: 0.1589\n",
            "Epoch [12/20], Avg Loss: 0.346414\n",
            "MAE: 0.019843, MSE: 0.000817, Move Acc: 0.4219, MRR sum_k: 0.2580\n",
            "Epoch [13/20], Avg Loss: 0.346827\n",
            "MAE: 0.018041, MSE: 0.000866, Move Acc: 0.4219, MRR sum_k: 0.2482\n",
            "Epoch [14/20], Avg Loss: 0.347507\n",
            "MAE: 0.017741, MSE: 0.000820, Move Acc: 0.5000, MRR sum_k: 0.1679\n",
            "Epoch [15/20], Avg Loss: 0.346853\n",
            "MAE: 0.020035, MSE: 0.000948, Move Acc: 0.5000, MRR sum_k: 0.1601\n",
            "Epoch [16/20], Avg Loss: 0.346885\n",
            "MAE: 0.020259, MSE: 0.000928, Move Acc: 0.3750, MRR sum_k: 0.1291\n",
            "Epoch [17/20], Avg Loss: 0.346941\n",
            "MAE: 0.017665, MSE: 0.000754, Move Acc: 0.5625, MRR sum_k: 0.1715\n",
            "Epoch [18/20], Avg Loss: 0.347278\n",
            "MAE: 0.019292, MSE: 0.000845, Move Acc: 0.5312, MRR sum_k: 0.2467\n",
            "Epoch [19/20], Avg Loss: 0.346607\n",
            "MAE: 0.016733, MSE: 0.000690, Move Acc: 0.5156, MRR sum_k: 0.1369\n",
            "Epoch [20/20], Avg Loss: 0.346649\n",
            "MAE: 0.016910, MSE: 0.000704, Move Acc: 0.5312, MRR sum_k: 0.1527\n",
            "Epoch [1/30], Avg Loss: 0.346309\n",
            "MAE: 0.015515, MSE: 0.000547, Move Acc: 0.5938, MRR sum_k: 0.2543\n",
            "Epoch [2/30], Avg Loss: 0.346985\n",
            "MAE: 0.016957, MSE: 0.000712, Move Acc: 0.4844, MRR sum_k: 0.3017\n",
            "Epoch [3/30], Avg Loss: 0.346616\n",
            "MAE: 0.018376, MSE: 0.000636, Move Acc: 0.4844, MRR sum_k: 0.4939\n",
            "Epoch [4/30], Avg Loss: 0.345968\n",
            "MAE: 0.014339, MSE: 0.000548, Move Acc: 0.5625, MRR sum_k: 0.2180\n",
            "Epoch [5/30], Avg Loss: 0.347165\n",
            "MAE: 0.016539, MSE: 0.000590, Move Acc: 0.4844, MRR sum_k: 0.4343\n",
            "Epoch [6/30], Avg Loss: 0.345515\n",
            "MAE: 0.018361, MSE: 0.000867, Move Acc: 0.3594, MRR sum_k: 0.1414\n",
            "Epoch [7/30], Avg Loss: 0.347049\n",
            "MAE: 0.015418, MSE: 0.000606, Move Acc: 0.4844, MRR sum_k: 0.1820\n",
            "Epoch [8/30], Avg Loss: 0.347703\n",
            "MAE: 0.020647, MSE: 0.000941, Move Acc: 0.5000, MRR sum_k: 0.1978\n",
            "Epoch [9/30], Avg Loss: 0.347290\n",
            "MAE: 0.017181, MSE: 0.000654, Move Acc: 0.3750, MRR sum_k: 0.1773\n",
            "Epoch [10/30], Avg Loss: 0.345636\n",
            "MAE: 0.019796, MSE: 0.000979, Move Acc: 0.3906, MRR sum_k: 0.1613\n",
            "Epoch [11/30], Avg Loss: 0.346370\n",
            "MAE: 0.019038, MSE: 0.000776, Move Acc: 0.4219, MRR sum_k: 0.1526\n",
            "Epoch [12/30], Avg Loss: 0.347209\n",
            "MAE: 0.018851, MSE: 0.000862, Move Acc: 0.5469, MRR sum_k: 0.2237\n",
            "Epoch [13/30], Avg Loss: 0.348051\n",
            "MAE: 0.016878, MSE: 0.000775, Move Acc: 0.4688, MRR sum_k: 0.3394\n",
            "Epoch [14/30], Avg Loss: 0.346681\n",
            "MAE: 0.018907, MSE: 0.000749, Move Acc: 0.4844, MRR sum_k: 0.1677\n",
            "Epoch [15/30], Avg Loss: 0.347192\n",
            "MAE: 0.018118, MSE: 0.000755, Move Acc: 0.5625, MRR sum_k: 0.1838\n",
            "Epoch [16/30], Avg Loss: 0.347554\n",
            "MAE: 0.019905, MSE: 0.000939, Move Acc: 0.4844, MRR sum_k: 0.2754\n",
            "Epoch [17/30], Avg Loss: 0.347921\n",
            "MAE: 0.015944, MSE: 0.000646, Move Acc: 0.5312, MRR sum_k: 0.3112\n",
            "Epoch [18/30], Avg Loss: 0.346252\n",
            "MAE: 0.020947, MSE: 0.000990, Move Acc: 0.4688, MRR sum_k: 0.3032\n",
            "Epoch [19/30], Avg Loss: 0.346086\n",
            "MAE: 0.019300, MSE: 0.000990, Move Acc: 0.5000, MRR sum_k: 0.1159\n",
            "Epoch [20/30], Avg Loss: 0.346503\n",
            "MAE: 0.015787, MSE: 0.000700, Move Acc: 0.4062, MRR sum_k: 0.1919\n",
            "Epoch [21/30], Avg Loss: 0.346478\n",
            "MAE: 0.016716, MSE: 0.000746, Move Acc: 0.5156, MRR sum_k: 0.0907\n",
            "Epoch [22/30], Avg Loss: 0.346305\n",
            "MAE: 0.020934, MSE: 0.000994, Move Acc: 0.4531, MRR sum_k: 0.1541\n",
            "Epoch [23/30], Avg Loss: 0.347234\n",
            "MAE: 0.017024, MSE: 0.000767, Move Acc: 0.5781, MRR sum_k: 0.2543\n",
            "Epoch [24/30], Avg Loss: 0.347587\n",
            "MAE: 0.014021, MSE: 0.000475, Move Acc: 0.5781, MRR sum_k: 0.2870\n",
            "Epoch [25/30], Avg Loss: 0.347759\n",
            "MAE: 0.020815, MSE: 0.001044, Move Acc: 0.5156, MRR sum_k: 0.1343\n",
            "Epoch [26/30], Avg Loss: 0.345987\n",
            "MAE: 0.015480, MSE: 0.000482, Move Acc: 0.5156, MRR sum_k: 0.2938\n",
            "Epoch [27/30], Avg Loss: 0.347703\n",
            "MAE: 0.019043, MSE: 0.000822, Move Acc: 0.5312, MRR sum_k: 0.1505\n",
            "Epoch [28/30], Avg Loss: 0.346643\n",
            "MAE: 0.017531, MSE: 0.000796, Move Acc: 0.4375, MRR sum_k: 0.1085\n",
            "Epoch [29/30], Avg Loss: 0.347117\n",
            "MAE: 0.018633, MSE: 0.000858, Move Acc: 0.5156, MRR sum_k: 0.2141\n",
            "Epoch [30/30], Avg Loss: 0.346105\n",
            "MAE: 0.016935, MSE: 0.000681, Move Acc: 0.5469, MRR sum_k: 0.1767\n",
            "Epoch [1/30], Avg Loss: 0.346611\n",
            "MAE: 0.020905, MSE: 0.000951, Move Acc: 0.4688, MRR sum_k: 0.1760\n",
            "Epoch [2/30], Avg Loss: 0.347654\n",
            "MAE: 0.020788, MSE: 0.000887, Move Acc: 0.4219, MRR sum_k: 0.1431\n",
            "Epoch [3/30], Avg Loss: 0.347131\n",
            "MAE: 0.016453, MSE: 0.000501, Move Acc: 0.5312, MRR sum_k: 0.2346\n",
            "Epoch [4/30], Avg Loss: 0.346321\n",
            "MAE: 0.019813, MSE: 0.001039, Move Acc: 0.5312, MRR sum_k: 0.1736\n",
            "Epoch [5/30], Avg Loss: 0.347238\n",
            "MAE: 0.016143, MSE: 0.000639, Move Acc: 0.5156, MRR sum_k: 0.0923\n",
            "Epoch [6/30], Avg Loss: 0.346950\n",
            "MAE: 0.018540, MSE: 0.000840, Move Acc: 0.4688, MRR sum_k: 0.1632\n",
            "Epoch [7/30], Avg Loss: 0.346626\n",
            "MAE: 0.018218, MSE: 0.000837, Move Acc: 0.5469, MRR sum_k: 0.1811\n",
            "Epoch [8/30], Avg Loss: 0.346918\n",
            "MAE: 0.017871, MSE: 0.000756, Move Acc: 0.4531, MRR sum_k: 0.2143\n",
            "Epoch [9/30], Avg Loss: 0.347025\n",
            "MAE: 0.015747, MSE: 0.000659, Move Acc: 0.5625, MRR sum_k: 0.1181\n",
            "Epoch [10/30], Avg Loss: 0.347582\n",
            "MAE: 0.017483, MSE: 0.000719, Move Acc: 0.4688, MRR sum_k: 0.2137\n",
            "Epoch [11/30], Avg Loss: 0.346095\n",
            "MAE: 0.016390, MSE: 0.000606, Move Acc: 0.4531, MRR sum_k: 0.2225\n",
            "Epoch [12/30], Avg Loss: 0.346387\n",
            "MAE: 0.016695, MSE: 0.000752, Move Acc: 0.4219, MRR sum_k: 0.3418\n",
            "Epoch [13/30], Avg Loss: 0.346651\n",
            "MAE: 0.019890, MSE: 0.000828, Move Acc: 0.5312, MRR sum_k: 0.2861\n",
            "Epoch [14/30], Avg Loss: 0.346768\n",
            "MAE: 0.020037, MSE: 0.000908, Move Acc: 0.5000, MRR sum_k: 0.1089\n",
            "Epoch [15/30], Avg Loss: 0.347648\n",
            "MAE: 0.020840, MSE: 0.001171, Move Acc: 0.4844, MRR sum_k: 0.1989\n",
            "Epoch [16/30], Avg Loss: 0.346665\n",
            "MAE: 0.022006, MSE: 0.001071, Move Acc: 0.5000, MRR sum_k: 0.1791\n",
            "Epoch [17/30], Avg Loss: 0.347117\n",
            "MAE: 0.018947, MSE: 0.000854, Move Acc: 0.4062, MRR sum_k: 0.1662\n",
            "Epoch [18/30], Avg Loss: 0.346609\n",
            "MAE: 0.019433, MSE: 0.000909, Move Acc: 0.5156, MRR sum_k: 0.5207\n",
            "Epoch [19/30], Avg Loss: 0.346119\n",
            "MAE: 0.018928, MSE: 0.000882, Move Acc: 0.5156, MRR sum_k: 0.2878\n",
            "Epoch [20/30], Avg Loss: 0.347388\n",
            "MAE: 0.017632, MSE: 0.000813, Move Acc: 0.5469, MRR sum_k: 0.1624\n",
            "Epoch [21/30], Avg Loss: 0.347734\n",
            "MAE: 0.015608, MSE: 0.000544, Move Acc: 0.5000, MRR sum_k: 0.2496\n",
            "Epoch [22/30], Avg Loss: 0.346879\n",
            "MAE: 0.017299, MSE: 0.000625, Move Acc: 0.4688, MRR sum_k: 0.1923\n",
            "Epoch [23/30], Avg Loss: 0.347077\n",
            "MAE: 0.016135, MSE: 0.000613, Move Acc: 0.6094, MRR sum_k: 0.5026\n",
            "Epoch [24/30], Avg Loss: 0.348425\n",
            "MAE: 0.018149, MSE: 0.000679, Move Acc: 0.5156, MRR sum_k: 0.2707\n",
            "Epoch [25/30], Avg Loss: 0.346541\n",
            "MAE: 0.016703, MSE: 0.000700, Move Acc: 0.5469, MRR sum_k: 0.1690\n",
            "Epoch [26/30], Avg Loss: 0.346411\n",
            "MAE: 0.018961, MSE: 0.000842, Move Acc: 0.4531, MRR sum_k: 0.3268\n",
            "Epoch [27/30], Avg Loss: 0.346827\n",
            "MAE: 0.022016, MSE: 0.000976, Move Acc: 0.4844, MRR sum_k: 0.3421\n",
            "Epoch [28/30], Avg Loss: 0.346435\n",
            "MAE: 0.019927, MSE: 0.000940, Move Acc: 0.4062, MRR sum_k: 0.1910\n",
            "Epoch [29/30], Avg Loss: 0.347797\n",
            "MAE: 0.017777, MSE: 0.000899, Move Acc: 0.4062, MRR sum_k: 0.2221\n",
            "Epoch [30/30], Avg Loss: 0.347257\n",
            "MAE: 0.021649, MSE: 0.001055, Move Acc: 0.5000, MRR sum_k: 0.2061\n",
            "Epoch [1/30], Avg Loss: 0.346467\n",
            "MAE: 0.018987, MSE: 0.000877, Move Acc: 0.5469, MRR sum_k: 0.2808\n",
            "Epoch [2/30], Avg Loss: 0.346854\n",
            "MAE: 0.018056, MSE: 0.000693, Move Acc: 0.4688, MRR sum_k: 0.1617\n",
            "Epoch [3/30], Avg Loss: 0.346794\n",
            "MAE: 0.020666, MSE: 0.000871, Move Acc: 0.4844, MRR sum_k: 0.1918\n",
            "Epoch [4/30], Avg Loss: 0.347056\n",
            "MAE: 0.019085, MSE: 0.000831, Move Acc: 0.5469, MRR sum_k: 0.3018\n",
            "Epoch [5/30], Avg Loss: 0.347067\n",
            "MAE: 0.020233, MSE: 0.000856, Move Acc: 0.4375, MRR sum_k: 0.1561\n",
            "Epoch [6/30], Avg Loss: 0.347279\n",
            "MAE: 0.019277, MSE: 0.000912, Move Acc: 0.4531, MRR sum_k: 0.3137\n",
            "Epoch [7/30], Avg Loss: 0.346749\n",
            "MAE: 0.021471, MSE: 0.000979, Move Acc: 0.5156, MRR sum_k: 0.0864\n",
            "Epoch [8/30], Avg Loss: 0.347343\n",
            "MAE: 0.018605, MSE: 0.000970, Move Acc: 0.3906, MRR sum_k: 0.1784\n",
            "Epoch [9/30], Avg Loss: 0.347235\n",
            "MAE: 0.021570, MSE: 0.000839, Move Acc: 0.5312, MRR sum_k: 0.3153\n",
            "Epoch [10/30], Avg Loss: 0.346368\n",
            "MAE: 0.016997, MSE: 0.000719, Move Acc: 0.6094, MRR sum_k: 0.2343\n",
            "Epoch [11/30], Avg Loss: 0.346813\n",
            "MAE: 0.022094, MSE: 0.001092, Move Acc: 0.5000, MRR sum_k: 0.1211\n",
            "Epoch [12/30], Avg Loss: 0.347619\n",
            "MAE: 0.017407, MSE: 0.000747, Move Acc: 0.5156, MRR sum_k: 0.1126\n",
            "Epoch [13/30], Avg Loss: 0.346944\n",
            "MAE: 0.019839, MSE: 0.000882, Move Acc: 0.5469, MRR sum_k: 0.1675\n",
            "Epoch [14/30], Avg Loss: 0.346944\n",
            "MAE: 0.015174, MSE: 0.000608, Move Acc: 0.4844, MRR sum_k: 0.1835\n",
            "Epoch [15/30], Avg Loss: 0.347264\n",
            "MAE: 0.017876, MSE: 0.000672, Move Acc: 0.5625, MRR sum_k: 0.1741\n",
            "Epoch [16/30], Avg Loss: 0.346980\n",
            "MAE: 0.017568, MSE: 0.000862, Move Acc: 0.5938, MRR sum_k: 0.2597\n",
            "Epoch [17/30], Avg Loss: 0.346666\n",
            "MAE: 0.016530, MSE: 0.000823, Move Acc: 0.4844, MRR sum_k: 0.1674\n",
            "Epoch [18/30], Avg Loss: 0.346439\n",
            "MAE: 0.018632, MSE: 0.000754, Move Acc: 0.4844, MRR sum_k: 0.1384\n",
            "Epoch [19/30], Avg Loss: 0.347237\n",
            "MAE: 0.015130, MSE: 0.000553, Move Acc: 0.5469, MRR sum_k: 0.2425\n",
            "Epoch [20/30], Avg Loss: 0.346397\n",
            "MAE: 0.016205, MSE: 0.000694, Move Acc: 0.4375, MRR sum_k: 0.1161\n",
            "Epoch [21/30], Avg Loss: 0.346868\n",
            "MAE: 0.018264, MSE: 0.000701, Move Acc: 0.5000, MRR sum_k: 0.2788\n",
            "Epoch [22/30], Avg Loss: 0.347615\n",
            "MAE: 0.015857, MSE: 0.000580, Move Acc: 0.5000, MRR sum_k: 0.2564\n",
            "Epoch [23/30], Avg Loss: 0.346599\n",
            "MAE: 0.016047, MSE: 0.000698, Move Acc: 0.3594, MRR sum_k: 0.1180\n",
            "Epoch [24/30], Avg Loss: 0.346552\n",
            "MAE: 0.016829, MSE: 0.000617, Move Acc: 0.4375, MRR sum_k: 0.1574\n",
            "Epoch [25/30], Avg Loss: 0.347133\n",
            "MAE: 0.015985, MSE: 0.000557, Move Acc: 0.6094, MRR sum_k: 0.1763\n",
            "Epoch [26/30], Avg Loss: 0.347241\n",
            "MAE: 0.021972, MSE: 0.001018, Move Acc: 0.5781, MRR sum_k: 0.1901\n",
            "Epoch [27/30], Avg Loss: 0.346308\n",
            "MAE: 0.016503, MSE: 0.000636, Move Acc: 0.5000, MRR sum_k: 0.1936\n",
            "Epoch [28/30], Avg Loss: 0.347584\n",
            "MAE: 0.016851, MSE: 0.000745, Move Acc: 0.5000, MRR sum_k: 0.2129\n",
            "Epoch [29/30], Avg Loss: 0.346597\n",
            "MAE: 0.020268, MSE: 0.000853, Move Acc: 0.3594, MRR sum_k: 0.1165\n",
            "Epoch [30/30], Avg Loss: 0.346790\n",
            "MAE: 0.019103, MSE: 0.000720, Move Acc: 0.5156, MRR sum_k: 0.1441\n",
            "\n",
            "🔍 Best Results:\n",
            "Best MAE: 0.019433\n",
            "Best MSE: 0.000909\n",
            "Best Movement Accuracy: 0.5156\n",
            "Best Epoch Count: 30, Best Batch Count: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"trained_stock_model.pth\"\n",
        "torch.save(model.state_dict(), model_path)\n",
        "print(f\"✅ Model saved to {model_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfy67JIWrIus",
        "outputId": "1b60bc02-5eff-4423-f657-51f18937355d"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model saved to trained_stock_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"trained_stock_model.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "nwDw78qPsSXv",
        "outputId": "e4abf6fc-5911-4b64-c14a-a0f01e3c8066"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_120c70f1-c547-4d60-8e51-8ea06660819f\", \"trained_stock_model.pth\", 359729)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Calculate MRR\n",
        "def calculate_mrr(predicted_ranking, actual_returns,k):\n",
        "    \"\"\"\n",
        "    Calculate Mean Reciprocal Rank (MRR)\n",
        "\n",
        "    Args:\n",
        "    predicted_ranking: List of tuples (stock_name, predicted_return) sorted by predicted_return\n",
        "    actual_returns: List of tuples (stock_name, actual_return)\n",
        "    \"\"\"\n",
        "    # Create a dictionary of actual returns for easy lookup\n",
        "    actual_dict = {name: ret for name, ret in actual_returns}\n",
        "\n",
        "    # Sort the actual returns to find the truly best performing stocks\n",
        "    actual_sorted = sorted(actual_returns, key=lambda x: x[1], reverse=True)\n",
        "    best_stocks = [name for name, ret in actual_sorted[:k]]  # Top 10 actual performers\n",
        "\n",
        "    reciprocal_ranks = []\n",
        "\n",
        "    for best_stock in best_stocks[:k]:\n",
        "        # Find the rank of this best stock in our predicted ranking\n",
        "        found = False\n",
        "        for rank, (stock_name, _) in enumerate(predicted_ranking, 1):  # Start from rank 1\n",
        "            if stock_name == best_stock:\n",
        "                reciprocal_ranks.append(1.0 / rank)\n",
        "                found = True\n",
        "                break\n",
        "\n",
        "        if not found:\n",
        "            reciprocal_ranks.append(0.0)\n",
        "\n",
        "    mrr = np.mean(reciprocal_ranks) if reciprocal_ranks else 0.0\n",
        "    return mrr\n",
        "\n",
        "\n",
        "def calculate_precision_at_k(predicted_ranking, actual_returns, k):\n",
        "    \"\"\"\n",
        "    Calculate Precision@K: Fraction of top-K predicted stocks that are actually among the top-K real performers.\n",
        "    \"\"\"\n",
        "    top_k_predicted = {name for name, _ in predicted_ranking[:k]}\n",
        "    top_k_actual = {name for name, _ in sorted(actual_returns, key=lambda x: x[1], reverse=True)[:k]}\n",
        "\n",
        "    correct_predictions = top_k_predicted.intersection(top_k_actual)\n",
        "    precision = len(correct_predictions) / k if k > 0 else 0.0\n",
        "    return precision\n",
        "\n",
        "\n",
        "def calculate_movement_accuracy(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculates the accuracy of predicted movement directions (up/down).\n",
        "    \"\"\"\n",
        "    y_true = np.array(y_true).flatten()\n",
        "    y_pred = np.array(y_pred).flatten()\n",
        "\n",
        "    # Determine direction of movements (1 for up, 0 for down)\n",
        "    true_direction = (y_true > 0).astype(int)\n",
        "    pred_direction = (y_pred > 0).astype(int)\n",
        "\n",
        "    accuracy = np.mean(true_direction == pred_direction)\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "def irr_at_k(true_returns_list, predicted_returns_list, k=5):\n",
        "    \"\"\"\n",
        "    Compute IRR@K from two lists of (stock, return) pairs.\n",
        "\n",
        "    Parameters:\n",
        "        true_returns_list (list): List of (stock, actual_return) tuples\n",
        "        predicted_returns_list (list): List of (stock, predicted_return) tuples\n",
        "        k (int): Top-K value for IRR@K\n",
        "\n",
        "    Returns:\n",
        "        irr (float): IRR@K value\n",
        "    \"\"\"\n",
        "    # Sort both lists by return descending\n",
        "    true_top_k = sorted(true_returns_list, key=lambda x: x[1], reverse=True)[:k]\n",
        "    pred_top_k = sorted(predicted_returns_list, key=lambda x: x[1], reverse=True)[:k]\n",
        "\n",
        "    # Create dict for quick lookup\n",
        "    true_dict = dict(true_returns_list)\n",
        "\n",
        "    # Compute sums\n",
        "    sum_true_top_k = sum([ret for _, ret in true_top_k])\n",
        "    sum_pred_top_k_actual_returns = sum([true_dict.get(stock, 0) for stock, _ in pred_top_k])\n",
        "\n",
        "    irr = sum_true_top_k - sum_pred_top_k_actual_returns\n",
        "    return irr\n"
      ],
      "metadata": {
        "id": "2MqVIWf3ocYm"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "e8YoWi2sdUnP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95d60181-db22-43a5-e378-169bb36daa32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 5 Predicted Stocks:\n",
            "Stock: AAVAS, Predicted Rank: 64, Actual Rank: 55\n",
            "Stock: ABB, Predicted Rank: 57, Actual Rank: 35\n",
            "Stock: ABSLAMC, Predicted Rank: 7, Actual Rank: 48\n",
            "Stock: ADANIGREEN, Predicted Rank: 20, Actual Rank: 1\n",
            "Stock: AIAENG, Predicted Rank: 50, Actual Rank: 37\n",
            "\n",
            "Top 10 Predicted Stocks:\n",
            "Stock: AAVAS, Predicted Rank: 64, Actual Rank: 55\n",
            "Stock: ABB, Predicted Rank: 57, Actual Rank: 35\n",
            "Stock: ABSLAMC, Predicted Rank: 7, Actual Rank: 48\n",
            "Stock: ADANIGREEN, Predicted Rank: 20, Actual Rank: 1\n",
            "Stock: AIAENG, Predicted Rank: 50, Actual Rank: 37\n",
            "Stock: ASTRAZEN, Predicted Rank: 39, Actual Rank: 22\n",
            "Stock: AUROPHARMA, Predicted Rank: 53, Actual Rank: 8\n",
            "Stock: BAJAJHLDNG, Predicted Rank: 18, Actual Rank: 39\n",
            "Stock: BANDHANBNK, Predicted Rank: 28, Actual Rank: 10\n",
            "Stock: BANKINDIA, Predicted Rank: 63, Actual Rank: 61\n",
            "\n",
            "Top 20 Predicted Stocks:\n",
            "Stock: AAVAS, Predicted Rank: 64, Actual Rank: 55\n",
            "Stock: ABB, Predicted Rank: 57, Actual Rank: 35\n",
            "Stock: ABSLAMC, Predicted Rank: 7, Actual Rank: 48\n",
            "Stock: ADANIGREEN, Predicted Rank: 20, Actual Rank: 1\n",
            "Stock: AIAENG, Predicted Rank: 50, Actual Rank: 37\n",
            "Stock: ASTRAZEN, Predicted Rank: 39, Actual Rank: 22\n",
            "Stock: AUROPHARMA, Predicted Rank: 53, Actual Rank: 8\n",
            "Stock: BAJAJHLDNG, Predicted Rank: 18, Actual Rank: 39\n",
            "Stock: BANDHANBNK, Predicted Rank: 28, Actual Rank: 10\n",
            "Stock: BANKINDIA, Predicted Rank: 63, Actual Rank: 61\n",
            "Stock: BATAINDIA, Predicted Rank: 23, Actual Rank: 6\n",
            "Stock: BLUEDART, Predicted Rank: 60, Actual Rank: 38\n",
            "Stock: BOSCHLTD, Predicted Rank: 48, Actual Rank: 46\n",
            "Stock: BRITANNIA, Predicted Rank: 1, Actual Rank: 45\n",
            "Stock: CANBK, Predicted Rank: 58, Actual Rank: 34\n",
            "Stock: CAPLIPOINT, Predicted Rank: 38, Actual Rank: 4\n",
            "Stock: CCL, Predicted Rank: 8, Actual Rank: 33\n",
            "Stock: CEATLTD, Predicted Rank: 4, Actual Rank: 49\n",
            "Stock: CENTRALBK, Predicted Rank: 27, Actual Rank: 19\n",
            "Stock: CHENNPETRO, Predicted Rank: 41, Actual Rank: 62\n"
          ]
        }
      ],
      "source": [
        "top_k_values = [5, 10, 20]\n",
        "\n",
        "for k in top_k_values:\n",
        "    top_k_predicted_stocks = sorted(best_predicted)[:k]\n",
        "    print(f\"\\nTop {k} Predicted Stocks:\")\n",
        "    for stock, _ in top_k_predicted_stocks:\n",
        "        print(f\"Stock: {stock}, Predicted Rank: {predicted_ranks[stock]}, Actual Rank: {actual_ranks[stock]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Sort predictions by predicted return descending\n",
        "predicted_ranking = sorted(best_predicted, key=lambda x: x[1], reverse=True)\n",
        "actual_returns_list = list(best_actual)\n",
        "# Define K values\n",
        "K = [5, 10, 20]\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "mrr_scores = [calculate_mrr(predicted_ranking, actual_returns_list, k) for k in K]\n",
        "precision_scores = [calculate_precision_at_k(predicted_ranking, actual_returns_list, k) for k in K]\n",
        "irr_scores = [irr_at_k(actual_returns_list, predicted_ranking, k) for k in K]\n",
        "\n",
        "# Display results\n",
        "print(\"Evaluation Metrics:\")\n",
        "print(f'MSE ={best_mse} | MAE = {best_mae} | Movement accuracy = {best_movement_accuracy}')\n",
        "for idx, k in enumerate(K):\n",
        "    print(f\"\\nk = {k}:\",end = '')\n",
        "    print(f\" MRR = {mrr_scores[idx]:.4f} |  Precision@{k} = {precision_scores[idx]:.4f} | IRR@{k} = {irr_scores[idx]:.4f}\")\n"
      ],
      "metadata": {
        "id": "tEdOLNQCoUCl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "471f28fd-a15d-4b73-9f0a-ff255f446885"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Metrics:\n",
            "MSE =0.0009090224241570248 | MAE = 0.01943349009843587 | Movement accuracy = 0.515625\n",
            "\n",
            "k = 5: MRR = 0.0866 |  Precision@5 = 0.2000 | IRR@5 = 0.1536\n",
            "\n",
            "k = 10: MRR = 0.0671 |  Precision@10 = 0.3000 | IRR@10 = 0.2188\n",
            "\n",
            "k = 20: MRR = 0.0594 |  Precision@20 = 0.2500 | IRR@20 = 0.3973\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EK7wISh0XuNN"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# from torch.optim import AdamW\n",
        "# from torch.optim.lr_scheduler import OneCycleLR\n",
        "# from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "# # Assuming model, loss_fn, intra_sector_stock_2_stock, sector_stocks are defined\n",
        "\n",
        "# # Custom Dataset Class\n",
        "# class StockDataset(Dataset):\n",
        "#     def __init__(self, data):\n",
        "#         self.data = data  # Replace with your actual data loading logic\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.data)  # Implement this based on your data\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         # Return your data sample here\n",
        "#         # For example:\n",
        "#         x, stock_names, y_true_move, y_true_return = self.data[idx]\n",
        "#         return x, stock_names, y_true_move, y_true_return\n",
        "\n",
        "# # Initialize dataset and data loader\n",
        "# dataset = StockDataset(your_data_source)  # Replace with actual data source\n",
        "# data_loader = DataLoader(dataset,\n",
        "#                         batch_size=32,  # Default batch size\n",
        "#                         shuffle=True,\n",
        "#                         num_workers=4)\n",
        "\n",
        "# # Optimizer and Scheduler\n",
        "# optimizer = AdamW(model.parameters(), lr=3e-4, weight_decay=1e-5)\n",
        "# scheduler = OneCycleLR(optimizer,\n",
        "#                       max_lr=1e-3,\n",
        "#                       steps_per_epoch=len(data_loader),\n",
        "#                       epochs=max([10, 20, 30]))\n",
        "\n",
        "# # Mixed Precision Training\n",
        "# scaler = GradScaler()\n",
        "\n",
        "# # Hyperparameters\n",
        "# maxlist = 0\n",
        "# epoch_list = [10,20,30]\n",
        "# batch_list = [4,8,16]\n",
        "# top_k = 5  # Number of top stocks to retrieve\n",
        "# top_k_values = [5, 10, 20]\n",
        "\n",
        "# # Training Loop\n",
        "# for num_epochs in epoch_list:\n",
        "#     for max_batches in batch_list:\n",
        "#         for epoch in range(num_epochs):\n",
        "#             model.train()\n",
        "#             running_loss = 0.0\n",
        "#             batch_count = 0\n",
        "#             stock_return_dict = {}\n",
        "#             stock_true_return_dict = {}\n",
        "\n",
        "#             for batch_idx, batch in enumerate(data_loader):\n",
        "#                 if batch_idx >= max_batches:\n",
        "#                     print(\"Reached max batch limit, stopping loop\")\n",
        "#                     break\n",
        "\n",
        "#                 batch_count += 1\n",
        "#                 x, stock_names, y_true_move, y_true_return = batch\n",
        "#                 x, y_true_move, y_true_return = x.to(device), y_true_move.to(device), y_true_return.to(device)\n",
        "\n",
        "#                 # Zero gradients\n",
        "#                 optimizer.zero_grad()\n",
        "\n",
        "#                 # Forward pass with mixed precision\n",
        "#                 with autocast():\n",
        "#                     output = model(x, stock_names, intra_sector_stock_2_stock, sector_stocks)\n",
        "#                     y_return, y_move, loss = loss_fn(output, y_true_return, y_true_move)\n",
        "\n",
        "#                 if torch.isnan(loss):\n",
        "#                     print(f\"NaN detected in loss at Epoch {epoch + 1}, Batch {batch_idx + 1}. Stopping training.\")\n",
        "#                     break\n",
        "\n",
        "#                 # Backward pass\n",
        "#                 scaler.scale(loss).backward()\n",
        "\n",
        "#                 # Gradient clipping\n",
        "#                 scaler.unscale_(optimizer)\n",
        "#                 torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "#                 # Update model parameters\n",
        "#                 scaler.step(optimizer)\n",
        "#                 scaler.update()\n",
        "\n",
        "#                 # Update running loss\n",
        "#                 running_loss += loss.detach().cpu().item()\n",
        "\n",
        "#                 # Store y_return and y_true_return for each stock\n",
        "#                 for i, stock in enumerate(stock_names):\n",
        "#                     stock_return_dict[stock] = y_return[i].detach().cpu().numpy()\n",
        "#                     stock_true_return_dict[stock] = y_true_return[i].detach().cpu().numpy()\n",
        "\n",
        "#             if batch_count == 0:\n",
        "#                 print(\"No batches processed, exiting training.\")\n",
        "#                 exit()\n",
        "\n",
        "#             avg_loss = running_loss / batch_count\n",
        "\n",
        "#             print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "#             # Rank stocks based on predicted and true returns\n",
        "#             sorted_predicted = sorted(stock_return_dict.items(), key=lambda item: item[1], reverse=True)\n",
        "#             sorted_actual = sorted(stock_true_return_dict.items(), key=lambda item: item[1], reverse=True)\n",
        "\n",
        "#             predicted_ranks = {stock: rank + 1 for rank, (stock, _) in enumerate(sorted_predicted)}\n",
        "#             actual_ranks = {stock: rank + 1 for rank, (stock, _) in enumerate(sorted_actual)}\n",
        "\n",
        "#             # Calculate MRR for top k stocks\n",
        "#             sum_k = 0\n",
        "#             for k in top_k_values:\n",
        "#                 top_k_predicted_stocks = sorted_predicted[:k]\n",
        "#                 mrr_top_k = sum(predicted_ranks[stock] / actual_ranks[stock] for stock, _ in top_k_predicted_stocks)\n",
        "#                 mrr_top_k /= k\n",
        "#                 sum_k += mrr_top_k\n",
        "\n",
        "#             if sum_k > maxlist:\n",
        "#                 maxlist = sum_k\n",
        "#                 maxbatch = max_batches\n",
        "#                 max_epochs = num_epochs\n",
        "\n",
        "#             # Validation phase every 5 epochs\n",
        "#             if (epoch + 1) % 5 == 0:\n",
        "#                 model.eval()\n",
        "#                 with torch.no_grad():\n",
        "#                     # Calculate validation metrics\n",
        "#                     validation_loss = 0\n",
        "#                     validation_mrr = 0\n",
        "#                     for batch_idx, batch in enumerate(data_loader):\n",
        "#                         x, stock_names, y_true_move, y_true_return = batch\n",
        "#                         x, y_true_move, y_true_return = x.to(device), y_true_move.to(device), y_true_return.to(device)\n",
        "#                         output = model(x, stock_names, intra_sector_stock_2_stock, sector_stocks)\n",
        "#                         y_return, y_move, loss = loss_fn(output, y_true_return, y_true_move)\n",
        "#                         validation_loss += loss.detach().cpu().item()\n",
        "\n",
        "#                         # Store y_return and y_true_return for each stock\n",
        "#                         stock_return_dict_val = {}\n",
        "#                         stock_true_return_dict_val = {}\n",
        "#                         for i, stock in enumerate(stock_names):\n",
        "#                             stock_return_dict_val[stock] = y_return[i].detach().cpu().numpy()\n",
        "#                             stock_true_return_dict_val[stock] = y_true_return[i].detach().cpu().numpy()\n",
        "\n",
        "#                         # Rank stocks based on predicted and true returns\n",
        "#                         sorted_predicted_val = sorted(stock_return_dict_val.items(), key=lambda item: item[1], reverse=True)\n",
        "#                         sorted_actual_val = sorted(stock_true_return_dict_val.items(), key=lambda item: item[1], reverse=True)\n",
        "\n",
        "#                         predicted_ranks_val = {stock: rank + 1 for rank, (stock, _) in enumerate(sorted_predicted_val)}\n",
        "#                         actual_ranks_val = {stock: rank + 1 for rank, (stock, _) in enumerate(sorted_actual_val)}\n",
        "\n",
        "#                         # Calculate MRR for top k stocks\n",
        "#                         top_k_predicted_stocks_val = sorted_predicted_val[:top_k]\n",
        "#                         mrr_top_k_val = sum(predicted_ranks_val[stock] / actual_ranks_val[stock] for stock, _ in top_k_predicted_stocks_val)\n",
        "#                         mrr_top_k_val /= top_k\n",
        "#                         validation_mrr += mrr_top_k_val\n",
        "\n",
        "#                     avg_validation_loss = validation_loss / len(data_loader)\n",
        "#                     avg_validation_mrr = validation_mrr / len(data_loader)\n",
        "#                     print(f\"Epoch [{epoch + 1}/{num_epochs}], Validation Loss: {avg_validation_loss:.4f}, Validation MRR: {avg_validation_mrr:.4f}\")\n",
        "\n",
        "#                 model.train()\n",
        "\n",
        "#             # Update scheduler\n",
        "#             scheduler.step()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}